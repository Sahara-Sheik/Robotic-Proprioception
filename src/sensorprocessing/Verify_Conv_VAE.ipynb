{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the Conv-VAE sensorprocessing\n",
    "\n",
    "Load a pre-trained model specified by an experiment/run trained by Train-Conv-VAE\n",
    "\n",
    "This notebook runs a number of visualizations that allow to illustrate the performance of the trained encoding. The verification here primarily happens through visual observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# adding the Julian-8897-Conv-VAE-PyTorch into the path\n",
    "\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "sys.path.append(Config()[\"conv_vae\"][\"code_dir\"])\n",
    "# from encoding_conv_vae.conv_vae import latest_json_and_model\n",
    "\n",
    "from sensorprocessing import sp_conv_vae\n",
    "from sensorprocessing import sp_helper\n",
    "from sensorprocessing.sp_helper import get_transform_to_sp, load_picturefile_to_tensor\n",
    "\n",
    "from demonstration.demonstration import Demonstration\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# At some point in the development, this hack was necessary for some reason. \n",
    "# It seems that as of Feb 2025, the code runs on Windows and Linux without it.\n",
    "#temp = pathlib.PosixPath\n",
    "#pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-run initialization\n",
    "Create the exp/run-s that describe the parameters of the training. \n",
    "Some of the code here is structured in such a way as to make the notebook automatizable with papermill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# *** Initialize the variables with default values \n",
    "# *** This cell should be tagged as parameters     \n",
    "# *** If papermill is used, some of the values will be overwritten\n",
    "\n",
    "experiment = \"sensorprocessing_conv_vae\"\n",
    "# run = \"sp_vae_256\" \n",
    "run = \"sp_vae_128\" \n",
    "\n",
    "# If it is set to true, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "# If not None, set an external experiment path\n",
    "external_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the necessary exp/run objects\n",
    "\n",
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path)\n",
    "    assert external_path.exists()\n",
    "    Config().set_experiment_path(external_path)\n",
    "\n",
    "# The experiment/run we are going to run: the specified model will be created\n",
    "exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n",
    "print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the Conv-VAE by visual reconstruction\n",
    "We can verify a Conv-VAE model visually based on its ability to recover the input image from the encoding. The intuition here would be that information that is lost during the recovery is not present in the encoding, and thus it won't be usable by the algorithms using this encoding either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sp_conv_vae.ConvVaeSensorProcessing(exp, device)\n",
    "transform = get_transform_to_sp(exp)\n",
    "demos = []\n",
    "cameras = []\n",
    "# load the demonstrations specified in the experiment validation data\n",
    "for val in exp[\"validation_data\"]:\n",
    "    run, demo_name, camera = val\n",
    "    exp_demo = Config().get_experiment(\"demonstration\", run)\n",
    "    demo = Demonstration(exp_demo, demo_name)\n",
    "    demos.append(demo)\n",
    "    cameras.append(camera)\n",
    "\n",
    "# Choose n pictures from the validation set and store them in lists of images and imagefiles\n",
    "n = 6\n",
    "demo = demos[0]\n",
    "camera = cameras[0]\n",
    "images_processable = []\n",
    "images_display = []\n",
    "#imagefiles = []\n",
    "for i in range(demo.metadata[\"maxsteps\"]):\n",
    "    rnd = random.randint(0, demo.metadata[\"maxsteps\"] - 1)\n",
    "    #imagefiles.append(demo.get_image_path(rnd))\n",
    "    image_processable, image_display = demo.get_image(rnd, device=device, camera=camera, transform=transform)\n",
    "    images_processable.append(image_processable)\n",
    "    images_display.append(image_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify whether the sp can process the images from the file\n",
    "# This is turned off, because most demonstrations will be in video \n",
    "# so no independent picture files\n",
    "transform = get_transform_to_sp(exp)\n",
    "for image in images_processable:\n",
    "    # sensor_readings, _ = load_picturefile_to_tensor(imagefile, transform, device)\n",
    "    output = sp.process(image)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify whether we can process the images loaded from the demonstration\n",
    "for image in images_processable:\n",
    "    z = sp.process(image)\n",
    "    print(f\"The encoding is\\n {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the VAE reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_VAE(sp, exp, device, image, axoriginal, axreconstr):\n",
    "    \"\"\"Helper function to show the original and the reconstruction in fields of a picture.\"\"\"\n",
    "    # transform = sp_helper.get_transform_to_sp(exp)\n",
    "    #input, image = sp_helper.load_picturefile_to_tensor(picture_name, transform, device=device)\n",
    "    # Running the input on the output\n",
    "    output, mu, logvar = sp.model(image)\n",
    "    # Output: the visual reconstruction\n",
    "    original = image[0].cpu().permute(1, 2, 0).detach().numpy()\n",
    "    output_for_pic = output[0].cpu().permute(1, 2, 0).detach().numpy()\n",
    "    # Showing the input and the reconstruction    \n",
    "    #axoriginal.imshow(image.to(\"cpu\").numpy()[0][0])\n",
    "    axoriginal.imshow(original)\n",
    "    axoriginal.set_title(\"Original\")\n",
    "    axreconstr.imshow(output_for_pic)\n",
    "    axreconstr.set_title(\"Reconstruct\")\n",
    "    return output, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell visualizes the original and reconstructed pictures by going inside \n",
    "# the sensorprocessing object and accessing the model\n",
    "fig, axs = plt.subplots(2, n, figsize=(10, 5))\n",
    "for i in range(n):\n",
    "    output, mu, logvar = visualize_VAE(sp, exp, device, images_processable[i], axs[0,i], axs[1,i])\n",
    "    print(f\"Pictures{i}\\nmu={mu}\\nlogvar={logvar}\")\n",
    "\n",
    "fig_reconstructed_path = pathlib.Path(exp.data_dir(), \"orig_reconstr.pdf\")\n",
    "fig.savefig(fig_reconstructed_path, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction from noisy latent encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, n, figsize=(10, 5))\n",
    "for i in range(n):\n",
    "    output, mu, logvar = visualize_VAE(sp, exp, device, images_processable[i], axs[0,i], axs[1,i])\n",
    "    # print(f\"Pictures{i}\\nmu={mu}\\nlogvar={logvar}\")\n",
    "    # this samples a new z with its logvar\n",
    "    z2 = sp.model.reparameterize(mu, logvar)\n",
    "    # adding some noise to the encoding (FIXME: add random noise)\n",
    "    for j in range(exp[\"latent_size\"]):\n",
    "        z2[0][j] = z2[0][j] + 0.001\n",
    "    #output2 = sp.model.decode(z2)\n",
    "    output2 = sp.model.decode(mu)\n",
    "    output_for_pic2 = output2[0].cpu().permute(1, 2, 0).detach().numpy()\n",
    "    axs[2,i].imshow(output_for_pic2)\n",
    "    axs[2,i].set_title(\"Noised\")\n",
    "\n",
    "fig_reconstruction_from_noisy_path = pathlib.Path(exp.data_dir(), \"reconstr_from_noisy.pdf\")\n",
    "fig.savefig(fig_reconstruction_from_noisy_path, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating random samples from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this can be actually done by just calling sp.model.sample!\n",
    "samples = sp.model.sample(num_samples = 25, current_device=device)\n",
    "fig, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
    "i = 0\n",
    "for x in range(0, 5):\n",
    "    for y in range(0, 5):\n",
    "        output_for_pic = samples[i].cpu().permute(1, 2, 0).detach().numpy()\n",
    "        axs[x][y].imshow(output_for_pic)\n",
    "        i += 1\n",
    "\n",
    "fig_random_samples = pathlib.Path(exp.data_dir(), \"random_samples.pdf\")\n",
    "fig.savefig(fig_random_samples, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
