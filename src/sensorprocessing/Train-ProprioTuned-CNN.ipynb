{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a proprioception-tuned CNN\n",
    "\n",
    "We create a sensor processing model using CNN-based visual encoding finetuned with proprioception.\n",
    "\n",
    "We create an encoding for the robot starting from a pretrained CNN model. As the feature vector of this is still large (eg 512 * 7 * 7), we reduce this to the encoding with an MLP. \n",
    "\n",
    "We finetune the encoding with information from proprioception.  \n",
    "\n",
    "The sensor processing object associated with the network trained like this is in sensorprocessing/sp_propriotuned_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config, Experiment\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from demonstration.demonstration import Demonstration\n",
    "\n",
    "import sensorprocessing.sp_helper as sp_helper\n",
    "from sensorprocessing.sp_propriotuned_cnn import VGG19ProprioTunedRegression, ResNetProprioTunedRegression\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Loading pointer config file:\n",
      "\tC:\\Users\\lboloni\\.config\\BerryPicker\\mainsettings.yaml\n",
      "***ExpRun**: Loading machine-specific config file:\n",
      "\tG:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\settings-LotziYoga.yaml\n",
      "***ExpRun**: No system dependent experiment file\n",
      "\t G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\sensorprocessing_propriotuned_cnn\\resnet50_256_sysdep.yaml,\n",
      "\t that is ok, proceeding.\n",
      "***ExpRun**: Configuration for exp/run: sensorprocessing_propriotuned_cnn/resnet50_256 successfully loaded\n",
      "***ExpRun**: No system dependent experiment file\n",
      "\t G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\robot_al5d\\position_controller_00_sysdep.yaml,\n",
      "\t that is ok, proceeding.\n",
      "***ExpRun**: Configuration for exp/run: robot_al5d/position_controller_00 successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# The experiment/run we are going to run: the specified model will be created\n",
    "experiment = \"sensorprocessing_propriotuned_cnn\"\n",
    "# run = \"vgg19_128\"\n",
    "# run = \"resnet50_128\"\n",
    "# run = \"vgg19_256\"\n",
    "run = \"resnet50_256\"\n",
    "exp = Config().get_experiment(experiment, run)\n",
    "\n",
    "exp_robot = Config().get_experiment(exp[\"robot_exp\"], exp[\"robot_run\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create regression training data (image to proprioception)\n",
    "The training data (X, Y) is all the pictures from a demonstration with the corresponding proprioception data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_as_proprioception_training(exp: Experiment, exp_robot: Experiment):\n",
    "    \"\"\"Loads the training images specified in the exp/run. Processes them as two tensors as input and target data for proprioception training. \n",
    "    Caches the processed results into the input and target file specified in the exp/run. \n",
    "    \n",
    "    Remove those files to recalculate\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    proprioception_input_path = pathlib.Path(exp.data_dir(), \"proprio_input.pth\")\n",
    "    proprioception_target_path = pathlib.Path(exp.data_dir(), \"proprio_target.pth\")\n",
    "\n",
    "    if proprioception_input_path.exists():\n",
    "        retval[\"inputs\"] = torch.load(proprioception_input_path, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_path, weights_only=True)\n",
    "    else:\n",
    "        inputlist = []\n",
    "        targetlist = []\n",
    "        transform = sp_helper.get_transform_to_sp(exp)\n",
    "        for val in exp[\"training_data\"]:\n",
    "            run, demo_name, camera = val\n",
    "            #run = val[0]\n",
    "            #demo_name = val[1]\n",
    "            #camera = val[2]\n",
    "            exp_demo = Config().get_experiment(\"demonstration\", run)\n",
    "            demo = Demonstration(exp_demo, demo_name)\n",
    "            for i in range(demo.metadata[\"maxsteps\"]):\n",
    "                sensor_readings, _ = demo.get_image(i, device=device, transform=transform, camera=camera)\n",
    "                inputlist.append(sensor_readings[0])\n",
    "                a = demo.get_action(i)\n",
    "                rp = RobotPosition.from_vector(exp_robot, a)\n",
    "                anorm = rp.to_normalized_vector(exp_robot)        \n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "        retval[\"inputs\"] = torch.stack(inputlist)\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "        torch.save(retval[\"inputs\"], proprioception_input_path)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_path)\n",
    "\n",
    "    # Separate the training and validation data. \n",
    "    # We will be shuffling the demonstrations \n",
    "    length = retval[\"inputs\"].size(0)\n",
    "    rows = torch.randperm(length) \n",
    "    shuffled_inputs = retval[\"inputs\"][rows]\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    training_size = int( length * 0.67 )\n",
    "    retval[\"inputs_training\"] = shuffled_inputs[1:training_size]\n",
    "    retval[\"targets_training\"] = shuffled_targets[1:training_size]\n",
    "\n",
    "    retval[\"inputs_validation\"] = shuffled_inputs[training_size:]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['freeform', '2024_12_26__16_40_20', 'dev2'],\n",
       " ['freeform', '2024_12_26__16_44_06', 'dev2']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp[\"training_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment default config C:\\Users\\lboloni\\Documents\\Code\\_Checkouts\\BerryPicker\\src\\experiment_configs\\demonstration\\_defaults_demonstration.yaml was empty, ok.\n",
      "***ExpRun**: No system dependent experiment file\n",
      "\t G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\demonstration\\freeform_sysdep.yaml,\n",
      "\t that is ok, proceeding.\n",
      "***ExpRun**: Configuration for exp/run: demonstration/freeform successfully loaded\n",
      "***ExpRun**: Experiment default config C:\\Users\\lboloni\\Documents\\Code\\_Checkouts\\BerryPicker\\src\\experiment_configs\\demonstration\\_defaults_demonstration.yaml was empty, ok.\n",
      "***ExpRun**: No system dependent experiment file\n",
      "\t G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\demonstration\\freeform_sysdep.yaml,\n",
      "\t that is ok, proceeding.\n",
      "***ExpRun**: Configuration for exp/run: demonstration/freeform successfully loaded\n"
     ]
    }
   ],
   "source": [
    "tr = load_images_as_proprioception_training(exp, exp_robot)\n",
    "inputs_training = tr[\"inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "inputs_validation = tr[\"inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model that performs proprioception regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if exp['model'] == 'VGG19ProprioTunedRegression':\n",
    "    model = VGG19ProprioTunedRegression(exp, device)\n",
    "elif exp['model'] == 'ResNetProprioTunedRegression':\n",
    "    model = ResNetProprioTunedRegression(exp, device)\n",
    "else:\n",
    "    raise Exception(f\"Unknown model {exp['model']}\")\n",
    "\n",
    "if exp['loss'] == 'MSELoss':        \n",
    "    criterion = nn.MSELoss()\n",
    "elif exp['loss'] == 'L1Loss':\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=exp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "batch_size = exp['batch_size']\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(model, criterion, optimizer, modelfile, device=\"cpu\", epochs=20):\n",
    "    \"\"\"Trains and saves the proprioception model\n",
    "    FIXME: must have parameters etc to investigate alternative models. \n",
    "    \"\"\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            predictions = model.forward(batch_X)\n",
    "            loss = criterion(predictions, batch_y)            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    torch.save(model.state_dict(), modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 9.7989\n",
      "Epoch [2/100], Loss: 5.1735\n",
      "Epoch [3/100], Loss: 2.0769\n",
      "Epoch [4/100], Loss: 1.3856\n",
      "Epoch [5/100], Loss: 1.2403\n",
      "Epoch [6/100], Loss: 1.0555\n",
      "Epoch [7/100], Loss: 0.9506\n",
      "Epoch [8/100], Loss: 0.8644\n",
      "Epoch [9/100], Loss: 0.7569\n",
      "Epoch [10/100], Loss: 0.8716\n",
      "Epoch [11/100], Loss: 1.1990\n",
      "Epoch [12/100], Loss: 1.0022\n",
      "Epoch [13/100], Loss: 1.0653\n",
      "Epoch [14/100], Loss: 0.7871\n",
      "Epoch [15/100], Loss: 0.6696\n",
      "Epoch [16/100], Loss: 0.6177\n",
      "Epoch [17/100], Loss: 0.6338\n",
      "Epoch [18/100], Loss: 0.5654\n",
      "Epoch [19/100], Loss: 0.8018\n",
      "Epoch [20/100], Loss: 0.6104\n",
      "Epoch [21/100], Loss: 0.4996\n",
      "Epoch [22/100], Loss: 0.4557\n",
      "Epoch [23/100], Loss: 0.7569\n",
      "Epoch [24/100], Loss: 0.6287\n",
      "Epoch [25/100], Loss: 0.4808\n",
      "Epoch [26/100], Loss: 0.4883\n",
      "Epoch [27/100], Loss: 0.5298\n",
      "Epoch [28/100], Loss: 0.5008\n",
      "Epoch [29/100], Loss: 0.4448\n",
      "Epoch [30/100], Loss: 0.4764\n",
      "Epoch [31/100], Loss: 0.4026\n",
      "Epoch [32/100], Loss: 0.5370\n",
      "Epoch [33/100], Loss: 0.4178\n",
      "Epoch [34/100], Loss: 0.4284\n",
      "Epoch [35/100], Loss: 0.4271\n",
      "Epoch [36/100], Loss: 0.3647\n",
      "Epoch [37/100], Loss: 0.3523\n",
      "Epoch [38/100], Loss: 0.4474\n",
      "Epoch [39/100], Loss: 0.3717\n",
      "Epoch [40/100], Loss: 0.3067\n",
      "Epoch [41/100], Loss: 0.2915\n",
      "Epoch [42/100], Loss: 0.4584\n",
      "Epoch [43/100], Loss: 0.3376\n",
      "Epoch [44/100], Loss: 0.2953\n",
      "Epoch [45/100], Loss: 0.3655\n",
      "Epoch [46/100], Loss: 0.3611\n",
      "Epoch [47/100], Loss: 0.4480\n",
      "Epoch [48/100], Loss: 0.3127\n",
      "Epoch [49/100], Loss: 0.2976\n",
      "Epoch [50/100], Loss: 0.3862\n",
      "Epoch [51/100], Loss: 0.4467\n",
      "Epoch [52/100], Loss: 0.3013\n",
      "Epoch [53/100], Loss: 0.3059\n",
      "Epoch [54/100], Loss: 0.2441\n",
      "Epoch [55/100], Loss: 0.3429\n",
      "Epoch [56/100], Loss: 0.2634\n",
      "Epoch [57/100], Loss: 0.3093\n",
      "Epoch [58/100], Loss: 0.2339\n",
      "Epoch [59/100], Loss: 0.2372\n",
      "Epoch [60/100], Loss: 0.2495\n",
      "Epoch [61/100], Loss: 0.2284\n",
      "Epoch [62/100], Loss: 0.3252\n",
      "Epoch [63/100], Loss: 0.4757\n",
      "Epoch [64/100], Loss: 0.1998\n",
      "Epoch [65/100], Loss: 0.3277\n",
      "Epoch [66/100], Loss: 0.2112\n",
      "Epoch [67/100], Loss: 0.3114\n",
      "Epoch [68/100], Loss: 0.2638\n",
      "Epoch [69/100], Loss: 0.3292\n",
      "Epoch [70/100], Loss: 0.3428\n",
      "Epoch [71/100], Loss: 0.2750\n",
      "Epoch [72/100], Loss: 0.2060\n",
      "Epoch [73/100], Loss: 0.1706\n",
      "Epoch [74/100], Loss: 0.1804\n",
      "Epoch [75/100], Loss: 0.3115\n",
      "Epoch [76/100], Loss: 0.3388\n",
      "Epoch [77/100], Loss: 0.3030\n",
      "Epoch [78/100], Loss: 0.2446\n",
      "Epoch [79/100], Loss: 0.1970\n",
      "Epoch [80/100], Loss: 0.1707\n",
      "Epoch [81/100], Loss: 0.1360\n",
      "Epoch [82/100], Loss: 0.1348\n",
      "Epoch [83/100], Loss: 0.1786\n",
      "Epoch [84/100], Loss: 0.1617\n",
      "Epoch [85/100], Loss: 0.2183\n",
      "Epoch [86/100], Loss: 0.2480\n",
      "Epoch [87/100], Loss: 0.1925\n",
      "Epoch [88/100], Loss: 0.1461\n",
      "Epoch [89/100], Loss: 0.2146\n",
      "Epoch [90/100], Loss: 0.1497\n",
      "Epoch [91/100], Loss: 0.1171\n",
      "Epoch [92/100], Loss: 0.2598\n",
      "Epoch [93/100], Loss: 0.2285\n",
      "Epoch [94/100], Loss: 0.1542\n",
      "Epoch [95/100], Loss: 0.1350\n",
      "Epoch [96/100], Loss: 0.1913\n",
      "Epoch [97/100], Loss: 0.2075\n",
      "Epoch [98/100], Loss: 0.2116\n",
      "Epoch [99/100], Loss: 0.1304\n",
      "Epoch [100/100], Loss: 0.1332\n",
      "Test Loss: 0.2823\n"
     ]
    }
   ],
   "source": [
    "modelfile = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "epochs = exp[\"epochs\"]\n",
    "if modelfile.exists():\n",
    "    print(\"*** Train-Propriotuned-CNN ***: NOT training; model already exists, loading it\")\n",
    "    model.load_state_dict(torch.load(modelfile))\n",
    "else:\n",
    "    train_and_save_proprioception_model(model, criterion, optimizer, modelfile, device=device, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
