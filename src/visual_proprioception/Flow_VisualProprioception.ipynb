{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bea14d",
   "metadata": {},
   "source": [
    "# Visual proprioception flow\n",
    "\n",
    "Create the full flow for training models for visual proprioception. This notebook programmatically generates a set of exp/runs that cover all the necessary components for a visual proprioception system (sensor processing,  visual proprioception regressor and verification notebooks).\n",
    "\n",
    "Then, it writes the exp/runs into an external directory full separated from the github source, and creates an automation script that runs them. A separate directory for the results is also created. \n",
    "\n",
    "Finally, it runs the necessary notebooks to execute the whole flow using papermill.\n",
    "\n",
    "The results directory contain the output of this flow, both in terms of trained models, as well as results (in the verification exp/run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8a83531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import copy\n",
    "import pprint\n",
    "import pathlib\n",
    "import yaml\n",
    "import tqdm\n",
    "import papermill\n",
    "import visproprio_helper \n",
    "from demonstration.demonstration import list_demos\n",
    "from demonstration.demopack import import_demopack, group_chooser_sp_vp_standard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b173f38",
   "metadata": {},
   "source": [
    "# Setting up the separate directory\n",
    "Setting up a separate directory for generated exp/run config files and the results. This cell will create a new directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ac9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Path for external experiments:\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\exprun\n",
      "***Path for external data:\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\result\n",
      "***ExpRun**: Experiment config path changed to c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\exprun\n",
      "***ExpRun**: Experiment data path changed to c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\result\n",
      "***ExpRun**: Experiment demonstration copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\exprun\\demonstration\n",
      "***ExpRun**: Experiment sensorprocessing_conv_vae copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\exprun\\sensorprocessing_conv_vae\n",
      "***ExpRun**: Experiment sensorprocessing_propriotuned_cnn copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\exprun\\sensorprocessing_propriotuned_cnn\n",
      "***ExpRun**: Experiment sensorprocessing_propriotuned_Vit copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\exprun\\sensorprocessing_propriotuned_Vit\n",
      "***ExpRun**: Experiment sensorprocessing_aruco copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\exprun\\sensorprocessing_aruco\n",
      "***ExpRun**: Experiment visual_proprioception copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\exprun\\visual_proprioception\n",
      "*** import_demopack: c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Demopacks\\automove-pack-01, target directory c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\result\\demonstration\\automove-pack-01  already exists, not copying\n",
      "***ExpRun**: Configuration for exp/run: demonstration/automove-pack-01 successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# the most likely changing things\n",
    "flow_name = \"VisualProprioception_flow_01\"\n",
    "#demonstration_run = \"touch-apple\"\n",
    "#demopack_name = \"touch-apple\"\n",
    "demopack_name = \"automove-pack-01\"\n",
    "demonstration_cam = \"dev2\"\n",
    "\n",
    "# determine these values based on experience\n",
    "# epochs_sp = 300\n",
    "# epochs_vp = 1000\n",
    "epochs_sp = 300\n",
    "epochs_vp = 1000\n",
    "image_size = [256, 256] # for vgg... etc\n",
    "\n",
    "# Use exist_ok not to-re-run previously successfully run models\n",
    "creation_style = \"exist-ok\"\n",
    "# creation_style = \"discard-old\"\n",
    "\n",
    "exprun_path, result_path = visproprio_helper.external_setup(flow_name, Config()[\"flows_path\"])\n",
    "\n",
    "demopack_path = pathlib.Path(Config()[\"demopacks_path\"], demopack_name)\n",
    "selection = import_demopack(demopack_path, group_chooser_sp_vp_standard)\n",
    "#\n",
    "# Configuring the training and validation data, based \n",
    "# on all the demonstrations of a particular type\n",
    "#\n",
    "experiment = \"demonstration\"\n",
    "exp = Config().get_experiment(experiment, demopack_name)\n",
    "\n",
    "sp_training_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"sp_training\"]]\n",
    "sp_validation_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"sp_validation\"]]\n",
    "vp_training_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"vp_training\"]]\n",
    "vp_validation_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"vp_validation\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be9a19e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sp_training': ['sp_training_00000', 'sp_training_00001', 'sp_training_00002', 'sp_training_00003'], 'sp_validation': ['sp_validation_00000', 'sp_validation_00001'], 'sp_testing': ['sp_testing_00000', 'sp_testing_00001'], 'vp_training': ['vp_training_00000', 'vp_training_00001', 'vp_training_00002', 'vp_training_00003'], 'vp_validation': ['vp_validation_00000', 'vp_validation_00001'], 'vp_testing': ['vp_testing_00000', 'vp_testing_00001']}\n"
     ]
    }
   ],
   "source": [
    "print(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca2ada4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sp_testing_00000', 'sp_testing_00001', 'sp_training_00000', 'sp_training_00001', 'sp_training_00002', 'sp_training_00003', 'sp_validation_00000', 'sp_validation_00001']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sp_training_00000',\n",
       " 'sp_training_00001',\n",
       " 'sp_training_00002',\n",
       " 'sp_training_00003']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demos = list_demos(exp)\n",
    "# print(demos)\n",
    "print(list_demos(exp, \"sp\"))\n",
    "[s for s in demos if s.startswith(\"sp_training\" + \"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6d27c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sp_conv_vae(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the conv-vae sensorprocessing with the right training data and parameters. Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation. \n",
    "    NOTE: a similar function is in Flow_BehaviorCloning.\n",
    "    \"\"\"\n",
    "    val = {}\n",
    "    val[\"latent_size\"] = params[\"latent_size\"]\n",
    "    val[\"epochs\"] = params[\"epochs\"]\n",
    "    val[\"save_period\"] = 5\n",
    "    val[\"training_data\"] = params[\"training_data\"]\n",
    "    val[\"validation_data\"] = params[\"validation_data\"]\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file \n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_Conv-VAE\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_Conv_VAE.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d97895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sp_propriotuned_cnn(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the propriotuned CNN with the right training data and parameters. \n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation. \n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"output_size\"] = 6\n",
    "    val[\"batch_size\"] = 32\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file \n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_CNN\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_ProprioTuned_CNN.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "476839eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vp_train(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training visual proprioception regressor.  \n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation. \n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    \n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file \n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Train_{run_name}\"\n",
    "    v[\"notebook\"] = \"visual_proprioception/Train_VisualProprioception.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b99c9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vp_verify(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the verification of the visual proprioception regressor.  \n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation. \n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    \n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file \n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Verify_{run_name}\"\n",
    "    v[\"notebook\"] = \"TODO Verify.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f64e6066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vp_compare(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the verification of the visual proprioception regressor.  \n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation. \n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"name\"] = exp_name\n",
    "\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file \n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Compare_{run_name}\"\n",
    "    v[\"notebook\"] = \"visual_proprioception/Compare_VisualProprioception.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28cd090",
   "metadata": {},
   "source": [
    "### Generate the exp/runs to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "020b8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensorprocessing_conv_vae sp_conv_vae_128_0001 128\n",
      "sensorprocessing_propriotuned_cnn sp_vgg19_128_0001 128\n",
      "sensorprocessing_propriotuned_cnn sp_resnet50_128_0001 128\n",
      "sensorprocessing_conv_vae sp_conv_vae_256_0001 256\n",
      "sensorprocessing_propriotuned_cnn sp_vgg19_256_0001 256\n",
      "sensorprocessing_propriotuned_cnn sp_resnet50_256_0001 256\n"
     ]
    }
   ],
   "source": [
    "expruns = []\n",
    "# overall values\n",
    "latent_sizes = [128, 256] # the possible latent sizes we consider\n",
    "cnntypes = [\"vgg19\", \"resnet50\"] # the CNN architectures we consider\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************\n",
    "# generate the sensorprocessing models\n",
    "# *******************************************\n",
    "sps = [] # the list of the sensorprocessing models (exp/run)\n",
    "for latent_size in latent_sizes:\n",
    "\n",
    "    # generate the vae exprun\n",
    "    exp_name = \"sensorprocessing_conv_vae\"\n",
    "    run_name = f\"sp_conv_vae_{latent_size}_0001\"\n",
    "    params = {}\n",
    "    params[\"latent_size\"] = latent_size\n",
    "    params[\"epochs\"] = epochs_sp\n",
    "    params[\"training_data\"] = sp_training_data\n",
    "    params[\"validation_data\"] = sp_validation_data\n",
    "    exprun = generate_sp_conv_vae(\n",
    "        exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name = run_name)\n",
    "    exprun[\"latent_size\"] = latent_size\n",
    "    sps.append(exprun)\n",
    "    expruns.append(exprun)\n",
    "\n",
    "    # generate the propriotuned expruns\n",
    "    for cnntype in cnntypes:\n",
    "        exp_name = \"sensorprocessing_propriotuned_cnn\"\n",
    "        run_name = f\"sp_{cnntype}_{latent_size}_0001\"\n",
    "        params = {}\n",
    "        params[\"image_size\"] = image_size\n",
    "        params[\"latent_size\"] = latent_size\n",
    "        params[\"epochs\"] = epochs_sp\n",
    "        params[\"training_data\"] = sp_training_data\n",
    "        params[\"validation_data\"] = sp_validation_data\n",
    "        if cnntype == \"vgg19\":\n",
    "            params[\"class\"] = \"VGG19ProprioTunedSensorProcessing\"\n",
    "            params[\"model\"] = \"VGG19ProprioTunedRegression\"\n",
    "        elif cnntype == \"resnet50\":\n",
    "            params[\"class\"] = \"ResNetProprioTunedSensorProcessing\"\n",
    "            params[\"model\"] = \"ResNetProprioTunedRegression\"\n",
    "            params[\"freeze_feature_extractor\"] = True\n",
    "            params[\"reductor_step_1\"] = 512\n",
    "            params[\"proprio_step_1\"] = 64\n",
    "            params[\"proprio_step_2\"] = 16\n",
    "        else:\n",
    "            raise Exception(f\"Unknown cnntype {cnntype}\")\n",
    "        params[\"loss\"] = \"MSELoss\" # alternative L1Loss\n",
    "        params[\"learning_rate\"] = 0.001\n",
    "        # alternative\n",
    "        exprun = generate_sp_propriotuned_cnn(\n",
    "            exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name = run_name)\n",
    "        exprun[\"latent_size\"] = latent_size\n",
    "        exprun[\"cnntype\"] = cnntype\n",
    "        sps.append(exprun)\n",
    "        expruns.append(exprun)\n",
    "\n",
    "    # FIXME: add here the ViT models\n",
    "\n",
    "# *******************************************\n",
    "# generate the proprioception models\n",
    "# *******************************************\n",
    "vpruns = []\n",
    "vpruns_latent = {128:[], 256:[]}\n",
    "for spexp, sprun,latent_size in [(a[\"experiment\"],a[\"run\"],a[\"latent_size\"]) for a in sps]:\n",
    "    print(spexp, sprun, latent_size)\n",
    "    # *** generate the vp train expruns ***\n",
    "    exp_name = \"visual_proprioception\"\n",
    "    run_name = \"vp_\" + sprun[3:]\n",
    "    vpruns.append(run_name)\n",
    "    vpruns_latent[latent_size].append(run_name)\n",
    "    params = {}\n",
    "    params[\"name\"] = run_name\n",
    "    params[\"output_size\"] = 6\n",
    "    params[\"encoding_size\"] = latent_size\n",
    "    params[\"training_data\"] = vp_training_data\n",
    "    params[\"validation_data\"] = vp_validation_data\n",
    "\n",
    "    params[\"regressor_hidden_size_1\"] = 64\n",
    "    params[\"regressor_hidden_size_1\"] = 64\n",
    "    params[\"loss\"] = \"MSE\"\n",
    "    params[\"epochs\"] = epochs_vp\n",
    "    params[\"batch_size\"] = 64\n",
    "    # FIXME this is hackish, should not do it this way\n",
    "    if \"vae\" in sprun.lower():\n",
    "        params[\"sensor_processing\"] = \"ConvVaeSensorProcessing\"\n",
    "    elif \"resnet\" in sprun.lower():\n",
    "        params[\"sensor_processing\"] = \"ResNetProprioTunedSensorProcessing\"\n",
    "    elif \"vgg19\" in sprun.lower():\n",
    "        params[\"sensor_processing\"] = \"VGG19ProprioTunedSensorProcessing\"\n",
    "    else:\n",
    "        raise Exception(f\"Unexpected sprun {sprun}\")\n",
    "\n",
    "    params[\"sp_experiment\"] = spexp\n",
    "    params[\"sp_run\"] = sprun\n",
    "\n",
    "    exprun = generate_vp_train(exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name=run_name)\n",
    "    # *** generate the vp verify expruns FIXME: not implemented yet ***\n",
    "    params_verify = {}\n",
    "    \n",
    "    expruns.append(exprun)\n",
    "# *******************************************\n",
    "# generate the comparisons: all, for latents 128 and 256\n",
    "# *******************************************\n",
    "exp_name = \"visual_proprioception\"\n",
    "# all\n",
    "run_name = \"vp_comp_flow_all\"\n",
    "params = {}\n",
    "params[\"name\"] = run_name\n",
    "params[\"tocompare\"] = vpruns\n",
    "exprun = generate_vp_compare(exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name=run_name)\n",
    "expruns.append(exprun)\n",
    "# by latent\n",
    "for latent_size in [128, 256]:\n",
    "    run_name = f\"vp_comp_flow_{latent_size}\"\n",
    "    params = {}\n",
    "    params[\"name\"] = run_name\n",
    "    params[\"tocompare\"] = vpruns_latent[latent_size]\n",
    "    exprun = generate_vp_compare(exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name=run_name)\n",
    "    expruns.append(exprun)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef389ef",
   "metadata": {},
   "source": [
    "### Run the flow\n",
    "\n",
    "Run the flow, that is, run a series of notebooks with papermill. In order to follow the execution inside these notebooks, one needs to open the output notebook, which is in the output_filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21aab023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Starting automated running of the flow.\n",
      " The path for the output notebooks is\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_01\\result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_Conv_VAE.ipynb :\n",
      " sensorprocessing_conv_vae/sp_conv_vae_128_0001\n",
      "--> Train_Conv_VAE_sensorprocessing_conv_vae_sp_conv_vae_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115b2f06d779404cafab80c4a3bbf9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/11 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:03<00:46,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_ProprioTuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_vgg19_128_0001\n",
      "--> Train_ProprioTuned_CNN_sensorprocessing_propriotuned_cnn_sp_vgg19_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540e93d7cd2448d4a71f50d3ff6d1939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/12 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:06<00:38,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_ProprioTuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_resnet50_128_0001\n",
      "--> Train_ProprioTuned_CNN_sensorprocessing_propriotuned_cnn_sp_resnet50_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eee972e9aea44a998ca3bf139fa62b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/12 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:08<00:33,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_Conv_VAE.ipynb :\n",
      " sensorprocessing_conv_vae/sp_conv_vae_256_0001\n",
      "--> Train_Conv_VAE_sensorprocessing_conv_vae_sp_conv_vae_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37b0720bdf045c7b21688c381593a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/11 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:11<00:32,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_ProprioTuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_vgg19_256_0001\n",
      "--> Train_ProprioTuned_CNN_sensorprocessing_propriotuned_cnn_sp_vgg19_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1592f8ee8c4d8f887928be9ea3f965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/12 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:14<00:28,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_ProprioTuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_resnet50_256_0001\n",
      "--> Train_ProprioTuned_CNN_sensorprocessing_propriotuned_cnn_sp_resnet50_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3475063a132f468b94e8fca671e9899b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/12 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:17<00:25,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_conv_vae_128_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_conv_vae_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b0101c330141fab10ffcb5f8a7df68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/15 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:21<00:24,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_vgg19_128_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_vgg19_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af1bd4d95c84aecaecc6d18fee371d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/15 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:25<00:23,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_resnet50_128_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_resnet50_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b73867fd83e4ba29cf1505fd4fd443e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/15 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:28<00:20,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_conv_vae_256_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_conv_vae_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95daf93dcaba46649d41108fd2fa77d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/15 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:32<00:17,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_vgg19_256_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_vgg19_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be165642785f40d78541f4f0f13b8e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/15 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:36<00:14,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_resnet50_256_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_resnet50_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00480d8d25e1489db3c95dde70687daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/15 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:39<00:10,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Compare_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_comp_flow_all\n",
      "--> Compare_VisualProprioception_visual_proprioception_vp_comp_flow_all_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17db8599633343649ed94fac5716331e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/18 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:48<00:10,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Compare_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_comp_flow_128\n",
      "--> Compare_VisualProprioception_visual_proprioception_vp_comp_flow_128_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb141b175631430881e4a8172810a7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/18 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:55<00:05,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Compare_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_comp_flow_256\n",
      "--> Compare_VisualProprioception_visual_proprioception_vp_comp_flow_256_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4191229610eb433bbe90150852001590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/18 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:02<00:00,  4.16s/it]\n"
     ]
    }
   ],
   "source": [
    "print(f\"***Starting automated running of the flow.\\n The path for the output notebooks is\\n{result_path}\")\n",
    "\n",
    "for exprun in tqdm.tqdm(expruns):\n",
    "    print(f\"***Automating {exprun['notebook']} :\\n {exprun['experiment']}/{exprun['run']}\")\n",
    "    notebook_path = pathlib.Path(\"..\", exprun[\"notebook\"])\n",
    "    output_filename = f\"{notebook_path.stem}_{exprun['experiment']}_{exprun['run']}_output{notebook_path.suffix}\"\n",
    "    print(f\"--> {output_filename}\")\n",
    "    # parameters that we are passing on to the notebook\n",
    "    params = {}\n",
    "    params[\"experiment\"] = exprun[\"experiment\"]\n",
    "    params[\"run\"] = exprun[\"run\"]\n",
    "    params[\"external_path\"] = exprun[\"external_path\"]\n",
    "    params[\"data_path\"] = exprun[\"data_path\"]    \n",
    "    output_path = pathlib.Path(result_path, output_filename)\n",
    "    try:\n",
    "        papermill.execute_notebook(\n",
    "            notebook_path,\n",
    "            output_path.absolute(),\n",
    "            cwd=notebook_path.parent,\n",
    "            parameters=params\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"There was an exception {e}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8455f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BerryPicker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
