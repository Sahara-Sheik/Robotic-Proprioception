{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad56c4b",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [5]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0268e3c",
   "metadata": {
    "papermill": {
     "duration": 0.003998,
     "end_time": "2025-11-18T10:07:36.733104",
     "exception": false,
     "start_time": "2025-11-18T10:07:36.729106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compare models for visual proprioception\n",
    "\n",
    "Compares regression models for visual proprioception, by running them on specific test data, and creating comparison graphs that put all of them onto the graphs. \n",
    "\n",
    "Each configuration is specified by a run of type visual_proprioception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd518b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:36.744645Z",
     "iopub.status.busy": "2025-11-18T10:07:36.744497Z",
     "iopub.status.idle": "2025-11-18T10:07:39.707066Z",
     "shell.execute_reply": "2025-11-18T10:07:39.705971Z"
    },
    "papermill": {
     "duration": 2.967627,
     "end_time": "2025-11-18T10:07:39.707610",
     "exception": false,
     "start_time": "2025-11-18T10:07:36.739983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/apps/anaconda/anaconda-2023.09/lib/python3.11/pathlib.py\n",
      "***ExpRun**: Loading pointer config file:\n",
      "\t/home/sa641631/.config/BerryPicker/mainsettings.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Loading machine-specific config file:\n",
      "\t~/WORK/BerryPicker/cfg/settings.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "# fixing the fact that it defaults to Type 3 fonts\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "#import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#from sensorprocessing import sp_conv_vae, sp_propriotuned_cnn, sp_vit\n",
    "\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training, load_multiview_demonstrations_as_proprioception_training\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "import sensorprocessing.sp_factory as sp_factory\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73a8038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:39.721936Z",
     "iopub.status.busy": "2025-11-18T10:07:39.721623Z",
     "iopub.status.idle": "2025-11-18T10:07:39.728357Z",
     "shell.execute_reply": "2025-11-18T10:07:39.727622Z"
    },
    "papermill": {
     "duration": 0.01239,
     "end_time": "2025-11-18T10:07:39.728827",
     "exception": false,
     "start_time": "2025-11-18T10:07:39.716437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Code for deterministic run, from Robi Konievic\n",
    "#\n",
    "superpower=777\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(superpower)\n",
    "import random\n",
    "random.seed(superpower)\n",
    "import numpy as np\n",
    "np.random.seed(superpower)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed_all(superpower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225a49ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:39.737486Z",
     "iopub.status.busy": "2025-11-18T10:07:39.737337Z",
     "iopub.status.idle": "2025-11-18T10:07:39.740753Z",
     "shell.execute_reply": "2025-11-18T10:07:39.739843Z"
    },
    "papermill": {
     "duration": 0.008453,
     "end_time": "2025-11-18T10:07:39.741223",
     "exception": false,
     "start_time": "2025-11-18T10:07:39.732770",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# If it is set to discard-old, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "experiment = \"visual_proprioception\"\n",
    "# run = \"vp_comp_flow_all\"\n",
    "run = \"vit_base\"\n",
    "\n",
    "# If not None, set the epochs to something different than the exp\n",
    "epochs = None\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "external_path = None\n",
    "#external_path = r\"C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\exprun\"\n",
    "# external_path = r\"C:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\cvpr_simulation_000\\exprun\"\n",
    "\n",
    "# If not None, set an output path\n",
    "data_path = None\n",
    "#data_path = r\"C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\result\"\n",
    "# data_path = r\"C:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\cvpr_simulation_000\\result\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdcffdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:39.749691Z",
     "iopub.status.busy": "2025-11-18T10:07:39.749542Z",
     "iopub.status.idle": "2025-11-18T10:07:39.752671Z",
     "shell.execute_reply": "2025-11-18T10:07:39.751728Z"
    },
    "papermill": {
     "duration": 0.008161,
     "end_time": "2025-11-18T10:07:39.753136",
     "exception": false,
     "start_time": "2025-11-18T10:07:39.744975",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "experiment = \"visual_proprioception\"\n",
    "run = \"vp_comp_flow_all\"\n",
    "external_path = \"~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun\"\n",
    "data_path = \"~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26405d50",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c065a0ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:39.762046Z",
     "iopub.status.busy": "2025-11-18T10:07:39.761904Z",
     "iopub.status.idle": "2025-11-18T10:07:41.572674Z",
     "shell.execute_reply": "2025-11-18T10:07:41.571387Z"
    },
    "papermill": {
     "duration": 1.81616,
     "end_time": "2025-11-18T10:07:41.573427",
     "exception": true,
     "start_time": "2025-11-18T10:07:39.757267",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment config path changed to ~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_aruco copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_aruco\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_conv_vae copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_conv_vae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_propriotuned_Vit copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_propriotuned_Vit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_propriotuned_cnn copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_propriotuned_cnn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment robot_al5d copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/robot_al5d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment demonstration copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/demonstration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment data path changed to ~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: visual_proprioception/vp_comp_flow_all successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: robot_al5d/position_controller_00 successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: robot_al5d/position_controller_00 successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: visual_proprioception/vit_base successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: sensorprocessing_propriotuned_Vit/vit_base successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ViT Sensor Processing:\n",
      "  Model: vit_b_16\n",
      "  Latent dimension: 128\n",
      "  Image size: [224, 224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vit_b_16 with output dimension 768\n",
      "Created projection network: 768 → 512 → 256 → 128\n",
      "Created latent representation: 768 → 512 → 128\n",
      "Created proprioceptor: 128 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Model file /home/sa641631/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result/sensorprocessing_propriotuned_Vit/vit_base/proprioception_mlp.pth does not exist. Using untrained model.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sa641631/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result/visual_proprioception/vit_base/proprioception_mlp.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m model = VisProprio_SimpleMLPRegression(subexp)\n\u001b[32m     42\u001b[39m modelfile = pathlib.Path(subexp[\u001b[33m\"\u001b[39m\u001b[33mdata_dir\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     43\u001b[39m                         subexp[\u001b[33m\"\u001b[39m\u001b[33mproprioception_mlp_model_file\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelfile\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     45\u001b[39m model.to(device)\n\u001b[32m     46\u001b[39m models.append(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/sa641631/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result/visual_proprioception/vit_base/proprioception_mlp.pth'"
     ]
    }
   ],
   "source": [
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path)\n",
    "    assert external_path.exists()\n",
    "    Config().set_exprun_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_aruco\")\n",
    "    Config().copy_experiment(\"sensorprocessing_conv_vae\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_Vit\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_cnn\")\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "if data_path:\n",
    "    data_path = pathlib.Path(data_path)\n",
    "    assert data_path.exists()\n",
    "    Config().set_results_path(data_path)\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)\n",
    "# runs = exp[\"tocompare\"]\n",
    "# run = \"vp_ptun_vgg19_128\"\n",
    "runs = [\"vit_base\"]\n",
    "# runs = [\"vp_convvae_128\", \"vp_convvae_256\", \"vp_ptun_vgg19_128\", \"vp_ptun_vgg19_256\", \"vp_ptun_resnet50_128\", \"vp_ptun_resnet50_256\", \"vp_aruco_128\"]\n",
    "\n",
    "\n",
    "exp_robot = Config().get_experiment(exp[\"robot_exp\"], exp[\"robot_run\"])\n",
    "subexps = [] # the sub experiments that we are comparing\n",
    "sps = [ ]\n",
    "spexps = [ ]\n",
    "models = [ ]\n",
    "# FIXME: it is not a good idea to hardwire this, later we should refer\n",
    "# from the comparison exprun\n",
    "exp_robot = Config().get_experiment(\"robot_al5d\", \"position_controller_00\")\n",
    "firstexp = None # the first experiment, this is where we get the data from\n",
    "for subrun in runs:\n",
    "    subexp = Config().get_experiment(experiment, subrun)\n",
    "    if firstexp is None:\n",
    "        firstexp = subexp\n",
    "    subexps.append(subexp)\n",
    "    spexp = Config().get_experiment(subexp[\"sp_experiment\"], subexp[\"sp_run\"])\n",
    "    spexps.append(spexp)\n",
    "    sp = sp_factory.create_sp(spexp, device)\n",
    "    sps.append(sp)\n",
    "    model = VisProprio_SimpleMLPRegression(subexp)\n",
    "    modelfile = pathlib.Path(subexp[\"data_dir\"],\n",
    "                            subexp[\"proprioception_mlp_model_file\"])\n",
    "    model.load_state_dict(torch.load(modelfile))\n",
    "    model.to(device)\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46514f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test data for the lead experiment\n",
    "# task = leadexp[\"proprioception_testing_task\"]\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    firstexp.data_dir(), firstexp[\"proprioception_test_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    firstexp.data_dir(), firstexp[\"proprioception_test_target_file\"])\n",
    "\n",
    "# Check if lead experiment is multi-view\n",
    "is_lead_multiview = exp.get(\"sensor_processing\", \"\") == \"Vit_multiview\" or firstexp.get(\"num_views\", 1) > 1\n",
    "\n",
    "if is_lead_multiview:\n",
    "    # Use multi-view loading function for the lead experiment\n",
    "    tr = load_multiview_demonstrations_as_proprioception_training(\n",
    "        task,\n",
    "        proprioception_input_file,\n",
    "        proprioception_target_file,\n",
    "        num_views=firstexp.get(\"num_views\", 2)\n",
    "    )\n",
    "else:\n",
    "    # Use original single-view loading function\n",
    "    # These are actually just using the last ones\n",
    "    tr = load_demonstrations_as_proprioception_training(\n",
    "        sps[0], firstexp, spexps[0], exp_robot, \"validation_data\", proprioception_input_file, proprioception_target_file, device=device\n",
    "    )\n",
    "\n",
    "\n",
    "# The targets should be the same regardless of single or multi-view\n",
    "targets = tr[\"targets\"]\n",
    "print(f\"There are {targets.shape[0]} target data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d91469f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Iterate through all the components. The thing with it though is that this will get the test data from the components, not from the lead exp.\n",
    "\n",
    "FIXME: maybe I could just write a function that runs a particular proprioceptor on a whole task and returns the y, and then just call that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd8b97",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_from = 0\n",
    "no_to = targets.shape[0]-1\n",
    "\n",
    "ypreds = []\n",
    "\n",
    "for subexp, sp, spexp, model in zip(subexps, sps, spexps, models):\n",
    "    # Make sure both sp and model are on the same device\n",
    "    model = model.to(device)\n",
    "    proprioception_input_file = pathlib.Path(\n",
    "        subexp.data_dir(), subexp[\"proprioception_test_input_file\"])\n",
    "    proprioception_target_file = pathlib.Path(\n",
    "        subexp.data_dir(), subexp[\"proprioception_test_target_file\"])\n",
    "\n",
    "    # Check if this is a multi-view model\n",
    "    is_multiview = subexp.get(\"sensor_processing\", \"\") == \"Vit_multiview\" or subexp.get(\"num_views\", 1) > 1\n",
    "\n",
    "    if is_multiview:\n",
    "        # Use the multi-view data loading function\n",
    "        tr = load_multiview_demonstrations_as_proprioception_training(\n",
    "            task,\n",
    "            proprioception_input_file,\n",
    "            proprioception_target_file,\n",
    "            num_views=subexp.get(\"num_views\", 2)\n",
    "        )\n",
    "\n",
    "        # Process each data point\n",
    "        ypred = []\n",
    "        y = []\n",
    "        t = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(no_from, no_to):\n",
    "                # Get the latent representation from sp (which handles the multi-view encoding)\n",
    "                views = [view[i].unsqueeze(0).to(device) for view in tr[\"view_inputs\"]]\n",
    "                latent = sp.process(views)\n",
    "\n",
    "                # Important: Make sure the latent tensor is on the same device as the model\n",
    "                latent_tensor = torch.from_numpy(latent).unsqueeze(0).to(device)\n",
    "                # Pass latent to regression model\n",
    "                predictions = model(latent_tensor)\n",
    "                # Append the data\n",
    "                t.append(i)\n",
    "                y.append(targets[i].numpy())\n",
    "                ypred.append(predictions[0].cpu().numpy())\n",
    "    else:\n",
    "        # Original single-view approach\n",
    "        tr = load_demonstrations_as_proprioception_training(\n",
    "            sp, subexp, spexp, exp_robot, \"validation_data\", proprioception_input_file, proprioception_target_file, device=device\n",
    "        )\n",
    "        inputs = tr[\"inputs\"]\n",
    "        ypred = []\n",
    "        y = []\n",
    "        t = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(no_from, no_to):\n",
    "                # Move input to the correct device\n",
    "                x = inputs[i].to(device)\n",
    "                predictions = model(torch.unsqueeze(x, dim=0))\n",
    "\n",
    "                # Move predictions back to CPU for numpy conversion\n",
    "                # Append the data\n",
    "                t.append(i)\n",
    "                y.append(targets[i].numpy())\n",
    "                ypred.append(predictions[0].cpu().numpy())\n",
    "\n",
    "    ypred = np.array(ypred)\n",
    "    ypreds.append(ypred)\n",
    "    y = np.array(y)\n",
    "    t = np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea5fbb6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c3a1b82",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Time compare plot - double column\n",
    "At this point, we should have the ypreds, the y and the t and we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f126724",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(8, 6))\n",
    "titles = [\"height\",\"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "for i in range(len(titles)):\n",
    "    ax = axs[i//3, i%3]\n",
    "    # ax.set_ylim(0, 1.4)\n",
    "    ax.plot(t, y[:,i], label=\"ground truth\")\n",
    "    for ypred, subexp in zip(ypreds,subexps):\n",
    "        # fixme, fix the label to the name in the exp\n",
    "        ax.plot(t, ypred[:,i], label=subexp[\"name\"])\n",
    "    if i==0:\n",
    "        fig.legend(bbox_to_anchor=(1.25, 1))\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"comparison.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891e554",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Time compare plot, single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21770874",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(7, 5.3))\n",
    "titles = [\"height\",\"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "for i in range(len(titles)):\n",
    "    ax = axs[i//3, i%3]\n",
    "    # ax.set_ylim(0, 2.0)\n",
    "    for ypred, subexp in zip(ypreds,subexps):\n",
    "        # fixme, fix the label to the name in the exp\n",
    "        ax.plot(t, ypred[:,i], label=subexp[\"name\"], linewidth=1)\n",
    "    ax.plot(t, y[:,i], label=\"ground truth\", linewidth=2, color=\"black\")\n",
    "    if i==2:\n",
    "        ax.legend()\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"comparison.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e5eb0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Time compare plot, vertical with legend bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c83c7e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fig, axs = plt.subplots(3,2, figsize=(5.3, 7))\n",
    "fig, axs = plt.subplots(3,2, figsize=(4.6, 6))\n",
    "titles = [\"height\",\"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "for i in range(len(titles)):\n",
    "    ax = axs[i//2, i%2]\n",
    "    #ax.set_ylim(0, 2.0)\n",
    "    for ypred, subexp in zip(ypreds,subexps):\n",
    "        # fixme, fix the label to the name in the exp\n",
    "        ax.plot(t, ypred[:,i], label=subexp[\"name\"], linewidth=1)\n",
    "    ax.plot(t, y[:,i], label=\"ground truth\", linewidth=2, color=\"black\")\n",
    "    if i==4:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, ncol=len(subexps)+1,\n",
    "            bbox_to_anchor=(0.5, 0), loc=\"upper center\")\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"comparison23.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"comparison23.jpg\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527da47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Comparing the average accuracy\n",
    "Create a graph that is comparing the average accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762611b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(4, 3))\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    ax = axs[i//3, i%3]\n",
    "    # ax.set_ylim(0, 0.4)\n",
    "    ax.set_ylim(0, 0.5)\n",
    "    # ax.plot(t, y[:,i], label=\"y\")\n",
    "    bars = []\n",
    "    names = []\n",
    "    for ypred, subexp in zip(ypreds,subexps):\n",
    "        ## FIXME: some kind of different plot\n",
    "        # error = avg(y[:,i], ypred[:,i])\n",
    "        error = math.sqrt(np.mean((y[:,i]- ypred[:,i]) ** 2))\n",
    "        br = ax.bar(subexp[\"name\"], error)\n",
    "        bars.append(br)\n",
    "        names.append(subexp[\"name\"])\n",
    "    # Remove x-axis labels if desired\n",
    "    ax.set_xticks([])\n",
    "    if i==0:\n",
    "        fig.legend(bars, names, bbox_to_anchor=(1.50, 0.9), ncol=1)\n",
    "    fig.tight_layout()\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"msecomparison.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"msecomparison.jpg\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d759df",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2, figsize=(4.6, 6))\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    ax = axs[i//2, i%2]\n",
    "    # ax.set_ylim(0, 0.4)\n",
    "    ax.set_ylim(0, 0.5)\n",
    "    # ax.plot(t, y[:,i], label=\"y\")\n",
    "    bars = []\n",
    "    names = []\n",
    "    for ypred, subexp in zip(ypreds,subexps):\n",
    "        ## FIXME: some kind of different plot\n",
    "        # error = avg(y[:,i], ypred[:,i])\n",
    "        error = math.sqrt(np.mean((y[:,i]- ypred[:,i]) ** 2))\n",
    "        br = ax.bar(subexp[\"name\"], error, label=subexp[\"name\"])\n",
    "        bars.append(br)\n",
    "        names.append(subexp[\"name\"])\n",
    "    # Remove x-axis labels if desired\n",
    "    ax.set_xticks([])\n",
    "    if i==4:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        #ncolumn = len(exps)\n",
    "        ncolumn = 2\n",
    "        fig.legend(handles, labels, ncol=ncolumn,\n",
    "            bbox_to_anchor=(0.5, 0), loc=\"upper center\")\n",
    "    # fig.tight_layout()\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"msecomparison23.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"msecomparison23.jpg\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d036480",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Generate csv file for further processing\n",
    "\n",
    "Code by Robert-Anton Konievic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15771d9a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(4, 3))\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    ax = axs[i//3, i%3]\n",
    "    # ax.set_ylim(0, 0.4)\n",
    "    ax.set_ylim(0, 0.5)\n",
    "    # ax.plot(t, y[:,i], label=\"y\")\n",
    "    bars = []\n",
    "    names = []\n",
    "    with open(pathlib.Path(exp[\"data_dir\"], \"msecomparison_values.txt\"),'a') as mse_values_file:\n",
    "        mse_values_file.write(titles[i] + \": \" + '\\n')\n",
    "    for ypred, subexp in zip(ypreds,subexps):\n",
    "        ## FIXME: some kind of different plot\n",
    "        # error = avg(y[:,i], ypred[:,i])\n",
    "        error = math.sqrt(np.mean((y[:,i]- ypred[:,i]) ** 2))\n",
    "        br = ax.bar(subexp[\"name\"], error)\n",
    "        bars.append(br)\n",
    "        names.append(subexp[\"name\"])\n",
    "        with open(pathlib.Path(exp[\"data_dir\"],\n",
    "            \"msecomparison_values.txt\"),'a') as mse_values_file:\n",
    "            mse_values_file.write(subexp[\"name\"] + \": \" + str(error) + '\\n')\n",
    "    # csv_path = pathlib.Path(exp[\"data_dir\"], \"all_msecomparison_values.csv\")\n",
    "    csv_path = pathlib.Path(exp[\"data_dir\"], \"all_msecomparison_values.csv\")\n",
    "\n",
    "\n",
    "# If you’re running this in a loop for multiple titles, open once per run\n",
    "# Use 'a' to append to the existing file, or 'w' to overwrite.\n",
    "    with open(csv_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write header row only if file is new/empty\n",
    "        if csv_path.stat().st_size == 0:\n",
    "            writer.writerow([\"Title\"] + [subexp[\"name\"] for subexp in subexps])\n",
    "         # Compute all errors for the current title\n",
    "        errors = []\n",
    "        for ypred, subexp in zip(ypreds, subexps):\n",
    "            error = math.sqrt(np.mean((y[:, i] - ypred[:, i]) ** 2))\n",
    "            errors.append(error)\n",
    "         # Write one row per title (e.g. per experiment)\n",
    "        writer.writerow([titles[i]] + errors)\n",
    "\n",
    "     # Remove x-axis labels if desired\n",
    "    ax.set_xticks([])\n",
    "    if i==0:\n",
    "        fig.legend(bars, names, bbox_to_anchor=(1.50, 0.9), ncol=1)\n",
    "    fig.tight_layout()\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"msecomparison-robi.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"msecomparison-robi.jpg\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BerryPicker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.87058,
   "end_time": "2025-11-18T10:07:42.633544",
   "environment_variables": {},
   "exception": true,
   "input_path": "../visual_proprioception/Compare_VisualProprioception.ipynb",
   "output_path": "/lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/visual_proprioception/~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result/Compare_VisualProprioception_visual_proprioception_vp_comp_flow_all_output.ipynb",
   "parameters": {
    "data_path": "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result",
    "experiment": "visual_proprioception",
    "external_path": "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun",
    "run": "vp_comp_flow_all"
   },
   "start_time": "2025-11-18T10:07:35.762964",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}