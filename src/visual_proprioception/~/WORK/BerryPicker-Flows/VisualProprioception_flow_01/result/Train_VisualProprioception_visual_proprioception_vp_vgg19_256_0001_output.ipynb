{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fecbbb2",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [5]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fb42f",
   "metadata": {
    "papermill": {
     "duration": 0.003563,
     "end_time": "2025-11-18T10:07:23.642465",
     "exception": false,
     "start_time": "2025-11-18T10:07:23.638902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train models for visual proprioception\n",
    "\n",
    "Train a regression model for visual proprioception. The input is sensory data (eg. a camera image). This is encoded by a p;predefined sensorprocessing component into a latent representation. What we are training and saving here is a regressor that is mapping the latent representation to the position of the robot (eg. a vector of 6 degrees of freedom).\n",
    "\n",
    "The specification of this regressor is specified in an experiment of the type \"visual_proprioception\". Running this notebook will train and save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f553a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:23.652757Z",
     "iopub.status.busy": "2025-11-18T10:07:23.652615Z",
     "iopub.status.idle": "2025-11-18T10:07:26.716012Z",
     "shell.execute_reply": "2025-11-18T10:07:26.715008Z"
    },
    "papermill": {
     "duration": 3.068111,
     "end_time": "2025-11-18T10:07:26.716537",
     "exception": false,
     "start_time": "2025-11-18T10:07:23.648426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/apps/anaconda/anaconda-2023.09/lib/python3.11/pathlib.py\n",
      "***ExpRun**: Loading pointer config file:\n",
      "\t/home/sa641631/.config/BerryPicker/mainsettings.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Loading machine-specific config file:\n",
      "\t~/WORK/BerryPicker/cfg/settings.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# from demonstration.demonstration import Demonstration\n",
    "\n",
    "\n",
    "from sensorprocessing.sp_factory import create_sp\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training, load_multiview_demonstrations_as_proprioception_training\n",
    "\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e584e5fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:26.727618Z",
     "iopub.status.busy": "2025-11-18T10:07:26.727327Z",
     "iopub.status.idle": "2025-11-18T10:07:26.733598Z",
     "shell.execute_reply": "2025-11-18T10:07:26.732470Z"
    },
    "papermill": {
     "duration": 0.01109,
     "end_time": "2025-11-18T10:07:26.734061",
     "exception": false,
     "start_time": "2025-11-18T10:07:26.722971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Code for deterministic run, from Robi Konievic\n",
    "#\n",
    "superpower=777\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(superpower)\n",
    "import random\n",
    "random.seed(superpower)\n",
    "import numpy as np\n",
    "np.random.seed(superpower)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed_all(superpower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879a2be",
   "metadata": {
    "papermill": {
     "duration": 0.003455,
     "end_time": "2025-11-18T10:07:26.741118",
     "exception": false,
     "start_time": "2025-11-18T10:07:26.737663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exp-run initialization\n",
    "Create the exp/run-s that describe the parameters of the training. \n",
    "Some of the code here is structured in such a way as to make the notebook automatizable with papermill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab110ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:26.748776Z",
     "iopub.status.busy": "2025-11-18T10:07:26.748627Z",
     "iopub.status.idle": "2025-11-18T10:07:26.752713Z",
     "shell.execute_reply": "2025-11-18T10:07:26.751784Z"
    },
    "papermill": {
     "duration": 0.00871,
     "end_time": "2025-11-18T10:07:26.753178",
     "exception": false,
     "start_time": "2025-11-18T10:07:26.744468",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# *** Initialize the variables with default values\n",
    "# *** This cell should be tagged as parameters\n",
    "# *** If papermill is used, some of the values will be overwritten\n",
    "\n",
    "# If it is set to discard-old, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "experiment = \"visual_proprioception\"\n",
    "\n",
    "# If not None, set the epochs to something different than the exp\n",
    "epochs = None\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "external_path = None\n",
    "\n",
    "# If not None, set an output path\n",
    "data_path = None\n",
    "\n",
    "# Dr. Boloni's path\n",
    "#external_path = pathlib.Path(Config()[\"experiment_external\"])\n",
    "# Sahara's path\n",
    "# external_path = pathlib.Path(\"/home/sa641631/SaharaBerryPickerData/experiment_data\")\n",
    "\n",
    "##############################################\n",
    "#                 SingleView                 #\n",
    "##############################################\n",
    "\n",
    "# the latent space 128 ones\n",
    "# run = \"vp_aruco_128\"\n",
    "# run = \"vp_convvae_128\"\n",
    "# run = \"vp_convvae_128\"\n",
    "run = \"vp_ptun_vgg19_128\"\n",
    "# run = \"vp_ptun_resnet50_128\"\n",
    "\n",
    "# the latent space 256 ones\n",
    "# run = \"vp_convvae_256\"\n",
    "# run = \"vp_ptun_vgg19_256\"\n",
    "# run = \"vp_ptun_resnet50_256\"\n",
    "\n",
    "#vits\n",
    "# run =\"vit_base\"\n",
    "# run =\"vit_large\"\n",
    "# run =\"vit_huge\"\n",
    "\n",
    "##############################################\n",
    "#                 MultiViews                 #\n",
    "##############################################\n",
    "\n",
    "#concat_proj\n",
    "\n",
    "# run =\"vit_base_multiview\"\n",
    "# run =\"vit_large_multiview\"\n",
    "# run =vit_huge_multiview\n",
    "\n",
    "\n",
    "##  indiv_proj\n",
    "# run = \"vit_base_multiview_indiv_proj\"  # ViT Base_indiv_proj\n",
    "# run = \"vit_large_multiview_indiv_proj\" # ViT Large_indiv_proj\n",
    "# run = \"vit_huge_multiview_indiv_proj\" # ViT Huge_indiv_proj\n",
    "\n",
    "##  attention\n",
    "# run = \"vit_base_multiview_attention\"  # ViT Base_attention\n",
    "# run = \"vit_large_multiview_attention\" # ViT Large_attention\n",
    "# run = \"vit_huge_multiview_attention\" # ViT Huge_attention\n",
    "\n",
    "\n",
    "##  weighted_sum\n",
    "# run = \"vit_base_multiview_weighted_sum\"  # ViT Base_weighted_sum\n",
    "# run = \"vit_large_multiview_weighted_sum\" # ViT Large_weighted_sum\n",
    "# run = \"vit_huge_multiview_weighted_sum\" # ViT Huge_weighted_sum\n",
    "\n",
    "##  gated\n",
    "# run = \"vit_base_multiview_gated\"  # ViT Base_gated\n",
    "# run = \"vit_large_multiview_gated\" # ViT Large_gated\n",
    "# run = \"vit_huge_multiview_gated\" # ViT Huge_gated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3689c458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:26.760971Z",
     "iopub.status.busy": "2025-11-18T10:07:26.760829Z",
     "iopub.status.idle": "2025-11-18T10:07:26.763875Z",
     "shell.execute_reply": "2025-11-18T10:07:26.763021Z"
    },
    "papermill": {
     "duration": 0.007423,
     "end_time": "2025-11-18T10:07:26.764339",
     "exception": false,
     "start_time": "2025-11-18T10:07:26.756916",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "experiment = \"visual_proprioception\"\n",
    "run = \"vp_vgg19_256_0001\"\n",
    "external_path = \"~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun\"\n",
    "data_path = \"~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed5785",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28d559e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:26.771935Z",
     "iopub.status.busy": "2025-11-18T10:07:26.771797Z",
     "iopub.status.idle": "2025-11-18T10:07:28.519885Z",
     "shell.execute_reply": "2025-11-18T10:07:28.518828Z"
    },
    "papermill": {
     "duration": 1.752923,
     "end_time": "2025-11-18T10:07:28.520672",
     "exception": true,
     "start_time": "2025-11-18T10:07:26.767749",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment config path changed to ~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_aruco copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_aruco\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_conv_vae copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_conv_vae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_propriotuned_Vit copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_propriotuned_Vit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_propriotuned_cnn copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_propriotuned_cnn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment robot_al5d copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/robot_al5d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment demonstration copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/demonstration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment visual_proprioception copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/visual_proprioception\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment data path changed to ~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: visual_proprioception/vp_vgg19_256_0001 successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment:\n",
      "    batch_size: 64\n",
      "    data_dir: /home/sa641631/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result/visual_proprioception/vp_vgg19_256_0001\n",
      "    encoding_size: 256\n",
      "    epochs: 1000\n",
      "    exp_run_sys_indep_file: ~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/visual_proprioception/vp_vgg19_256_0001.yaml\n",
      "    experiment_name: visual_proprioception\n",
      "    loss: MSE\n",
      "    name: vp_vgg19_256_0001\n",
      "    output_size: 6\n",
      "    proprioception_input_file: train_inputs.pt\n",
      "    proprioception_mlp_model_file: proprioception_mlp.pth\n",
      "    proprioception_target_file: train_targets.pt\n",
      "    proprioception_test_input_file: test_inputs.pt\n",
      "    proprioception_test_target_file: test_targets.pt\n",
      "    regressor_hidden_size_1: 64\n",
      "    regressor_hidden_size_2: 64\n",
      "    robot_exp: robot_al5d\n",
      "    robot_run: position_controller_00\n",
      "    run_name: vp_vgg19_256_0001\n",
      "    sensor_processing: VGG19ProprioTunedSensorProcessing\n",
      "    sp_experiment: sensorprocessing_propriotuned_cnn\n",
      "    sp_run: sp_vgg19_256_0001\n",
      "    subrun_name: null\n",
      "    time_started: '2025-11-18 05:07:26.970937'\n",
      "    training_data:\n",
      "    - - random-both-cameras-video\n",
      "      - vp_training_00000\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - vp_training_00001\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - vp_training_00002\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - vp_training_00003\n",
      "      - dev2\n",
      "    validation_data:\n",
      "    - - random-both-cameras-video\n",
      "      - vp_validation_00000\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - vp_validation_00001\n",
      "      - dev2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: sensorprocessing_propriotuned_cnn/sp_vgg19_256_0001 successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa641631/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sa641631/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Create the sp object described in the experiment\u001b[39;00m\n\u001b[32m     21\u001b[39m spexp = Config().get_experiment(exp[\u001b[33m\"\u001b[39m\u001b[33msp_experiment\u001b[39m\u001b[33m\"\u001b[39m], exp[\u001b[33m\"\u001b[39m\u001b[33msp_run\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m sp = \u001b[43mcreate_sp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m exp_robot = Config().get_experiment(exp[\u001b[33m\"\u001b[39m\u001b[33mrobot_exp\u001b[39m\u001b[33m\"\u001b[39m], exp[\u001b[33m\"\u001b[39m\u001b[33mrobot_run\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/visual_proprioception/../sensorprocessing/sp_factory.py:20\u001b[39m, in \u001b[36mcreate_sp\u001b[39m\u001b[34m(spexp, device)\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sp_conv_vae_concat_multiview.ConvVaeSensorProcessing_multiview(spexp, device)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spexp[\u001b[33m'\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m]==\u001b[33m\"\u001b[39m\u001b[33mVGG19ProprioTunedSensorProcessing\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msp_propriotuned_cnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVGG19ProprioTunedSensorProcessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spexp[\u001b[33m'\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m]==\u001b[33m\"\u001b[39m\u001b[33mResNetProprioTunedSensorProcessing\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sp_propriotuned_cnn.ResNetProprioTunedSensorProcessing(spexp, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/visual_proprioception/../sensorprocessing/sp_propriotuned_cnn.py:170\u001b[39m, in \u001b[36mVGG19ProprioTunedSensorProcessing.__init__\u001b[39m\u001b[34m(self, exp, device)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28mself\u001b[39m.enc = \u001b[38;5;28mself\u001b[39m.enc.to(device)\n\u001b[32m    168\u001b[39m modelfile = pathlib.Path(exp[\u001b[33m\"\u001b[39m\u001b[33mdata_dir\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m    169\u001b[39m                         exp[\u001b[33m\"\u001b[39m\u001b[33mproprioception_mlp_model_file\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m modelfile.exists()\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m.enc.load_state_dict(torch.load(modelfile))\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path)\n",
    "    assert external_path.exists()\n",
    "    Config().set_exprun_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_aruco\")\n",
    "    Config().copy_experiment(\"sensorprocessing_conv_vae\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_Vit\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_cnn\")\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "    Config().copy_experiment(\"visual_proprioception\")\n",
    "if data_path:\n",
    "    data_path = pathlib.Path(data_path)\n",
    "    assert data_path.exists()\n",
    "    Config().set_results_path(data_path)\n",
    "\n",
    "exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n",
    "pprint(exp)\n",
    "\n",
    "# Create the sp object described in the experiment\n",
    "spexp = Config().get_experiment(exp[\"sp_experiment\"], exp[\"sp_run\"])\n",
    "sp = create_sp(spexp, device)\n",
    "exp_robot = Config().get_experiment(exp[\"robot_exp\"], exp[\"robot_run\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa88caf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the regression model\n",
    "\n",
    "model = VisProprio_SimpleMLPRegression(exp)\n",
    "model.to(device)\n",
    "if exp[\"loss\"] == \"MSE\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif exp[\"loss\"] == \"L1\":\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    raise Exception(f'Unknown loss type {exp[\"loss\"]}')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f08b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Create the training and validation data which maps latent encodings into robot position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc1d00",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the original loading function\n",
    "\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_target_file\"])\n",
    "\n",
    "tr = load_demonstrations_as_proprioception_training(\n",
    "    sp, exp, spexp, exp_robot, \"training_data\", proprioception_input_file, proprioception_target_file, device=device\n",
    ")\n",
    "\n",
    "inputs_training = tr[\"inputs\"]\n",
    "targets_training = tr[\"targets\"]\n",
    "\n",
    "proprioception_test_input_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_test_input_file\"])\n",
    "proprioception_test_target_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_test_target_file\"])\n",
    "\n",
    "\n",
    "exp.start_timer(\"load-demos-as-proprioception-training\")\n",
    "tr_test = load_demonstrations_as_proprioception_training(\n",
    "    sp, exp, spexp, exp_robot, \"validation_data\", proprioception_test_input_file, proprioception_test_target_file, device=device\n",
    ")\n",
    "exp.end_timer(\"load-demos-as-proprioception-training\")\n",
    "\n",
    "inputs_validation = tr_test[\"inputs\"]\n",
    "targets_validation = tr_test[\"targets\"]\n",
    "\n",
    "# Create standard DataLoaders for single-view data\n",
    "batch_size = exp.get('batch_size', 32)\n",
    "# batch_size = exp['batch_size']\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08715865",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a385e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(exp):\n",
    "    \"\"\"Trains and saves the proprioception model, handling both single and multi-view inputs\n",
    "    with checkpoint support for resuming interrupted training\n",
    "    \"\"\"\n",
    "    final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "    checkpoint_dir = pathlib.Path(exp[\"data_dir\"], \"checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Maximum number of checkpoints to keep (excluding the best model)\n",
    "    max_checkpoints = 2\n",
    "\n",
    "    # Check if we're using a multi-view approach\n",
    "    is_multiview = exp.get(\"sensor_processing\", \"\").endswith(\"_multiview\") or exp.get(\"num_views\", 1) > 1\n",
    "    num_views = exp.get(\"num_views\", 2)\n",
    "\n",
    "    # First check for existing final model\n",
    "    if final_modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "        print(f\"Loading existing final model from {final_modelfile}\")\n",
    "        model.load_state_dict(torch.load(final_modelfile, map_location=device))\n",
    "\n",
    "        # Evaluate the loaded model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for batch_data in test_loader:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # Process the batch for evaluation\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                        sample_features = sp.process(sample_views)\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "            avg_loss = total_loss / max(batch_count, 1)\n",
    "            print(f\"Loaded model evaluation loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Function to extract epoch number from checkpoint file\n",
    "    def get_epoch_number(checkpoint_file):\n",
    "        try:\n",
    "            # Use a more robust approach to extract epoch number\n",
    "            # Format: epoch_XXXX.pth where XXXX is the epoch number\n",
    "            filename = checkpoint_file.stem\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                return int(parts[1])  # Get the number after \"epoch_\"\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    # Function to clean up old checkpoints\n",
    "    def cleanup_old_checkpoints():\n",
    "        # Get all epoch checkpoint files\n",
    "        checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "\n",
    "        # Sort by actual epoch number, not just filename\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        if len(checkpoint_files) > max_checkpoints:\n",
    "            files_to_delete = checkpoint_files[:-max_checkpoints]\n",
    "            for file_path in files_to_delete:\n",
    "                try:\n",
    "                    file_path.unlink()\n",
    "                    print(f\"Deleted old checkpoint: {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {file_path.name}: {e}\")\n",
    "\n",
    "    # Make sure model is on the correct device\n",
    "    model.to(device)\n",
    "    print(f\"Model moved to {device}\")\n",
    "\n",
    "    # Set training parameters\n",
    "    num_epochs = exp[\"epochs\"]\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # Check for existing checkpoints to resume from\n",
    "    checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number for more reliable ordering\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        # Get the most recent checkpoint\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        epoch_num = get_epoch_number(latest_checkpoint)\n",
    "\n",
    "        print(f\"Found checkpoint from epoch {epoch_num}. Resuming training...\")\n",
    "\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', float('inf'))\n",
    "\n",
    "        print(f\"Resuming from epoch {start_epoch}/{num_epochs} with best loss: {best_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Starting new training for {num_epochs} epochs\")\n",
    "\n",
    "    # Start or resume training\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Training loop handles both single and multi-view cases\n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            try:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # With multi-view, batch_views is a list of tensors, each with shape [batch_size, C, H, W]\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    # Process each sample in the batch\n",
    "                    for i in range(batch_size):\n",
    "                        # Extract this sample's views\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "\n",
    "                        # Process this sample through sp\n",
    "                        sample_features = sp.process(sample_views)\n",
    "\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    # Stack all samples' features into a batch\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    # Move to device\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    # Standard single-view processing\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "                # Print progress every few batches\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                # Save emergency checkpoint in case of error - use formatted epoch and batch numbers\n",
    "                save_path = checkpoint_dir / f\"emergency_epoch_{epoch:06d}_batch_{batch_idx:06d}.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_idx,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': total_loss / max(batch_count, 1),\n",
    "                    'best_loss': best_loss\n",
    "                }, save_path)\n",
    "                print(f\"Emergency checkpoint saved to {save_path}\")\n",
    "                continue\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = total_loss / max(batch_count, 1)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        eval_batch_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_data in test_loader:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # Process the batch the same way as in training\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                        sample_features = sp.process(sample_views)\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                test_loss += loss.item()\n",
    "                eval_batch_count += 1\n",
    "\n",
    "        avg_test_loss = test_loss / max(eval_batch_count, 1)\n",
    "        print(f'Validation Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "        # Save checkpoint after each epoch - using formatted epoch numbers for reliable sorting\n",
    "        checkpoint_path = checkpoint_dir / f\"epoch_{epoch:06d}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_loss,\n",
    "            'test_loss': avg_test_loss,\n",
    "            'best_loss': best_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        # Clean up old checkpoints to save space\n",
    "        cleanup_old_checkpoints()\n",
    "\n",
    "        # Update best model if improved\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_loss,\n",
    "                'test_loss': avg_test_loss,\n",
    "                'best_loss': best_loss\n",
    "            }, best_model_path)\n",
    "            print(f\"New best model saved with test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Training completed successfully\n",
    "    print(f\"Training complete. Best test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Load the best model for final save\n",
    "    best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "    if best_model_path.exists():\n",
    "        best_checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {best_checkpoint['epoch']+1} with test loss {best_checkpoint['test_loss']:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), final_modelfile)\n",
    "    print(f\"Final model saved to {final_modelfile}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411600ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\n",
    "\n",
    "#if modelfile.exists():\n",
    "#    model.load_state_dict(torch.load(modelfile))\n",
    "#else:\n",
    "exp.start_timer(\"train\")\n",
    "train_and_save_proprioception_model(exp)\n",
    "exp.end_timer(\"train\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f0a42",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp[\"timer-load-demos-as-proprioception-training-end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3696d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c10851",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BerryPicker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.494191,
   "end_time": "2025-11-18T10:07:29.482420",
   "environment_variables": {},
   "exception": true,
   "input_path": "../visual_proprioception/Train_VisualProprioception.ipynb",
   "output_path": "/lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/visual_proprioception/~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result/Train_VisualProprioception_visual_proprioception_vp_vgg19_256_0001_output.ipynb",
   "parameters": {
    "data_path": "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result",
    "experiment": "visual_proprioception",
    "external_path": "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun",
    "run": "vp_vgg19_256_0001"
   },
   "start_time": "2025-11-18T10:07:22.988229",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}