{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb59681a",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [5]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1142c",
   "metadata": {
    "papermill": {
     "duration": 0.003806,
     "end_time": "2025-11-18T10:07:18.052299",
     "exception": false,
     "start_time": "2025-11-18T10:07:18.048493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train models for visual proprioception\n",
    "\n",
    "Train a regression model for visual proprioception. The input is sensory data (eg. a camera image). This is encoded by a p;predefined sensorprocessing component into a latent representation. What we are training and saving here is a regressor that is mapping the latent representation to the position of the robot (eg. a vector of 6 degrees of freedom).\n",
    "\n",
    "The specification of this regressor is specified in an experiment of the type \"visual_proprioception\". Running this notebook will train and save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c4b267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:18.062430Z",
     "iopub.status.busy": "2025-11-18T10:07:18.062283Z",
     "iopub.status.idle": "2025-11-18T10:07:21.082253Z",
     "shell.execute_reply": "2025-11-18T10:07:21.081272Z"
    },
    "papermill": {
     "duration": 3.024681,
     "end_time": "2025-11-18T10:07:21.082796",
     "exception": false,
     "start_time": "2025-11-18T10:07:18.058115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/apps/anaconda/anaconda-2023.09/lib/python3.11/pathlib.py\n",
      "***ExpRun**: Loading pointer config file:\n",
      "\t/home/sa641631/.config/BerryPicker/mainsettings.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Loading machine-specific config file:\n",
      "\t~/WORK/BerryPicker/cfg/settings.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# from demonstration.demonstration import Demonstration\n",
    "\n",
    "\n",
    "from sensorprocessing.sp_factory import create_sp\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training, load_multiview_demonstrations_as_proprioception_training\n",
    "\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "900e0582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:21.093523Z",
     "iopub.status.busy": "2025-11-18T10:07:21.093242Z",
     "iopub.status.idle": "2025-11-18T10:07:21.099057Z",
     "shell.execute_reply": "2025-11-18T10:07:21.098341Z"
    },
    "papermill": {
     "duration": 0.010724,
     "end_time": "2025-11-18T10:07:21.099735",
     "exception": false,
     "start_time": "2025-11-18T10:07:21.089011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Code for deterministic run, from Robi Konievic\n",
    "#\n",
    "superpower=777\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(superpower)\n",
    "import random\n",
    "random.seed(superpower)\n",
    "import numpy as np\n",
    "np.random.seed(superpower)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed_all(superpower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f48bc0",
   "metadata": {
    "papermill": {
     "duration": 0.003529,
     "end_time": "2025-11-18T10:07:21.106826",
     "exception": false,
     "start_time": "2025-11-18T10:07:21.103297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exp-run initialization\n",
    "Create the exp/run-s that describe the parameters of the training. \n",
    "Some of the code here is structured in such a way as to make the notebook automatizable with papermill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b78ff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:21.114660Z",
     "iopub.status.busy": "2025-11-18T10:07:21.114511Z",
     "iopub.status.idle": "2025-11-18T10:07:21.118463Z",
     "shell.execute_reply": "2025-11-18T10:07:21.117659Z"
    },
    "papermill": {
     "duration": 0.008849,
     "end_time": "2025-11-18T10:07:21.118927",
     "exception": false,
     "start_time": "2025-11-18T10:07:21.110078",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# *** Initialize the variables with default values\n",
    "# *** This cell should be tagged as parameters\n",
    "# *** If papermill is used, some of the values will be overwritten\n",
    "\n",
    "# If it is set to discard-old, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "experiment = \"visual_proprioception\"\n",
    "\n",
    "# If not None, set the epochs to something different than the exp\n",
    "epochs = None\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "external_path = None\n",
    "\n",
    "# If not None, set an output path\n",
    "data_path = None\n",
    "\n",
    "# Dr. Boloni's path\n",
    "#external_path = pathlib.Path(Config()[\"experiment_external\"])\n",
    "# Sahara's path\n",
    "# external_path = pathlib.Path(\"/home/sa641631/SaharaBerryPickerData/experiment_data\")\n",
    "\n",
    "##############################################\n",
    "#                 SingleView                 #\n",
    "##############################################\n",
    "\n",
    "# the latent space 128 ones\n",
    "# run = \"vp_aruco_128\"\n",
    "# run = \"vp_convvae_128\"\n",
    "# run = \"vp_convvae_128\"\n",
    "run = \"vp_ptun_vgg19_128\"\n",
    "# run = \"vp_ptun_resnet50_128\"\n",
    "\n",
    "# the latent space 256 ones\n",
    "# run = \"vp_convvae_256\"\n",
    "# run = \"vp_ptun_vgg19_256\"\n",
    "# run = \"vp_ptun_resnet50_256\"\n",
    "\n",
    "#vits\n",
    "# run =\"vit_base\"\n",
    "# run =\"vit_large\"\n",
    "# run =\"vit_huge\"\n",
    "\n",
    "##############################################\n",
    "#                 MultiViews                 #\n",
    "##############################################\n",
    "\n",
    "#concat_proj\n",
    "\n",
    "# run =\"vit_base_multiview\"\n",
    "# run =\"vit_large_multiview\"\n",
    "# run =vit_huge_multiview\n",
    "\n",
    "\n",
    "##  indiv_proj\n",
    "# run = \"vit_base_multiview_indiv_proj\"  # ViT Base_indiv_proj\n",
    "# run = \"vit_large_multiview_indiv_proj\" # ViT Large_indiv_proj\n",
    "# run = \"vit_huge_multiview_indiv_proj\" # ViT Huge_indiv_proj\n",
    "\n",
    "##  attention\n",
    "# run = \"vit_base_multiview_attention\"  # ViT Base_attention\n",
    "# run = \"vit_large_multiview_attention\" # ViT Large_attention\n",
    "# run = \"vit_huge_multiview_attention\" # ViT Huge_attention\n",
    "\n",
    "\n",
    "##  weighted_sum\n",
    "# run = \"vit_base_multiview_weighted_sum\"  # ViT Base_weighted_sum\n",
    "# run = \"vit_large_multiview_weighted_sum\" # ViT Large_weighted_sum\n",
    "# run = \"vit_huge_multiview_weighted_sum\" # ViT Huge_weighted_sum\n",
    "\n",
    "##  gated\n",
    "# run = \"vit_base_multiview_gated\"  # ViT Base_gated\n",
    "# run = \"vit_large_multiview_gated\" # ViT Large_gated\n",
    "# run = \"vit_huge_multiview_gated\" # ViT Huge_gated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ea0d57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:21.127186Z",
     "iopub.status.busy": "2025-11-18T10:07:21.127033Z",
     "iopub.status.idle": "2025-11-18T10:07:21.132136Z",
     "shell.execute_reply": "2025-11-18T10:07:21.131246Z"
    },
    "papermill": {
     "duration": 0.009892,
     "end_time": "2025-11-18T10:07:21.132617",
     "exception": false,
     "start_time": "2025-11-18T10:07:21.122725",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "experiment = \"visual_proprioception\"\n",
    "run = \"vp_conv_vae_256_0001\"\n",
    "external_path = \"~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun\"\n",
    "data_path = \"~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338be2a5",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034a962d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T10:07:21.140371Z",
     "iopub.status.busy": "2025-11-18T10:07:21.140230Z",
     "iopub.status.idle": "2025-11-18T10:07:21.908038Z",
     "shell.execute_reply": "2025-11-18T10:07:21.906634Z"
    },
    "papermill": {
     "duration": 0.772743,
     "end_time": "2025-11-18T10:07:21.908830",
     "exception": true,
     "start_time": "2025-11-18T10:07:21.136087",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment config path changed to ~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_aruco copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_aruco\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_conv_vae copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_conv_vae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_propriotuned_Vit copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_propriotuned_Vit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment sensorprocessing_propriotuned_cnn copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/sensorprocessing_propriotuned_cnn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment robot_al5d copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/robot_al5d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment demonstration copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/demonstration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment visual_proprioception copied to\n",
      "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/visual_proprioception\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment data path changed to ~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: visual_proprioception/vp_conv_vae_256_0001 successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment:\n",
      "    batch_size: 64\n",
      "    data_dir: /home/sa641631/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result/visual_proprioception/vp_conv_vae_256_0001\n",
      "    encoding_size: 256\n",
      "    epochs: 1000\n",
      "    exp_run_sys_indep_file: ~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun/visual_proprioception/vp_conv_vae_256_0001.yaml\n",
      "    experiment_name: visual_proprioception\n",
      "    loss: MSE\n",
      "    name: vp_conv_vae_256_0001\n",
      "    output_size: 6\n",
      "    proprioception_input_file: train_inputs.pt\n",
      "    proprioception_mlp_model_file: proprioception_mlp.pth\n",
      "    proprioception_target_file: train_targets.pt\n",
      "    proprioception_test_input_file: test_inputs.pt\n",
      "    proprioception_test_target_file: test_targets.pt\n",
      "    regressor_hidden_size_1: 64\n",
      "    regressor_hidden_size_2: 64\n",
      "    robot_exp: robot_al5d\n",
      "    robot_run: position_controller_00\n",
      "    run_name: vp_conv_vae_256_0001\n",
      "    sensor_processing: ConvVaeSensorProcessing\n",
      "    sp_experiment: sensorprocessing_conv_vae\n",
      "    sp_run: sp_conv_vae_256_0001\n",
      "    subrun_name: null\n",
      "    time_started: '2025-11-18 05:07:21.343657'\n",
      "    training_data:\n",
      "    - - random-both-cameras-video\n",
      "      - vp_training_00000\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - vp_training_00001\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - vp_training_00002\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - vp_training_00003\n",
      "      - dev2\n",
      "    validation_data:\n",
      "    - - random-both-cameras-video\n",
      "      - vp_validation_00000\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - vp_validation_00001\n",
      "      - dev2\n",
      "\n",
      "***ExpRun**: Configuration for exp/run: sensorprocessing_conv_vae/sp_conv_vae_256_0001 successfully loaded\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sa641631/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result/sensorprocessing_conv_vae/sp_conv_vae_256_0001/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Create the sp object described in the experiment\u001b[39;00m\n\u001b[32m     21\u001b[39m spexp = Config().get_experiment(exp[\u001b[33m\"\u001b[39m\u001b[33msp_experiment\u001b[39m\u001b[33m\"\u001b[39m], exp[\u001b[33m\"\u001b[39m\u001b[33msp_run\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m sp = \u001b[43mcreate_sp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m exp_robot = Config().get_experiment(exp[\u001b[33m\"\u001b[39m\u001b[33mrobot_exp\u001b[39m\u001b[33m\"\u001b[39m], exp[\u001b[33m\"\u001b[39m\u001b[33mrobot_run\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/visual_proprioception/../sensorprocessing/sp_factory.py:14\u001b[39m, in \u001b[36mcreate_sp\u001b[39m\u001b[34m(spexp, device)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# spexp = Config().get_experiment(exp['sp_experiment'], exp['sp_run'])\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spexp[\u001b[33m\"\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mConvVaeSensorProcessing\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msp_conv_vae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConvVaeSensorProcessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spexp[\u001b[33m\"\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mConvVaeSensorProcessing_concat_multiview\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sp_conv_vae_concat_multiview.ConcatConvVaeSensorProcessing(spexp, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/visual_proprioception/../sensorprocessing/sp_conv_vae.py:63\u001b[39m, in \u001b[36mConvVaeSensorProcessing.__init__\u001b[39m\u001b[34m(self, exp, device)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.resume_model_pthfile = Path(exp.data_dir(), \u001b[33m\"\u001b[39m\u001b[33mmodel.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# self.conv_vae_jsonfile = conv_vae_jsonfile\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# self.resume_model_pthfile = resume_model_pthfile\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28mself\u001b[39m.vae_config = \u001b[43mget_conv_vae_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_vae_jsonfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresume_model_pthfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43minference_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# build model architecture\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.vae_config.init_obj(\u001b[33m'\u001b[39m\u001b[33march\u001b[39m\u001b[33m'\u001b[39m, module_arch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/visual_proprioception/../sensorprocessing/conv_vae.py:217\u001b[39m, in \u001b[36mget_conv_vae_config\u001b[39m\u001b[34m(jsonfile, resume_model, inference_only)\u001b[39m\n\u001b[32m    215\u001b[39m savedargv = sys.argv\n\u001b[32m    216\u001b[39m sys.argv = value\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m config = \u001b[43mConfigParser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m sys.argv = savedargv\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# print(json.dumps(config.config, indent=4))\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# THIS was an attempt to fix some kind of weird bug where an empty \u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# directory was created... it is not needed on 2024.11.17???\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# if it is inference only, remove the superfluously created directories.\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/src/Conv-VAE-PyTorch/parse_config.py:71\u001b[39m, in \u001b[36mConfigParser.from_args\u001b[39m\u001b[34m(cls, args, options)\u001b[39m\n\u001b[32m     68\u001b[39m     resume = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     69\u001b[39m     cfg_fname = Path(args.config)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m config = \u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.config \u001b[38;5;129;01mand\u001b[39;00m resume:\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# update new config for fine-tuning\u001b[39;00m\n\u001b[32m     74\u001b[39m     config.update(read_json(args.config))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/src/Conv-VAE-PyTorch/utils/util.py:16\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(fname)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_json\u001b[39m(fname):\n\u001b[32m     15\u001b[39m     fname = Path(fname)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m     17\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m json.load(handle, object_hook=OrderedDict)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/anaconda/anaconda-2023.09/lib/python3.11/pathlib.py:1044\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1043\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m io.open(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/sa641631/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result/sensorprocessing_conv_vae/sp_conv_vae_256_0001/config.json'"
     ]
    }
   ],
   "source": [
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path)\n",
    "    assert external_path.exists()\n",
    "    Config().set_exprun_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_aruco\")\n",
    "    Config().copy_experiment(\"sensorprocessing_conv_vae\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_Vit\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_cnn\")\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "    Config().copy_experiment(\"visual_proprioception\")\n",
    "if data_path:\n",
    "    data_path = pathlib.Path(data_path)\n",
    "    assert data_path.exists()\n",
    "    Config().set_results_path(data_path)\n",
    "\n",
    "exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n",
    "pprint(exp)\n",
    "\n",
    "# Create the sp object described in the experiment\n",
    "spexp = Config().get_experiment(exp[\"sp_experiment\"], exp[\"sp_run\"])\n",
    "sp = create_sp(spexp, device)\n",
    "exp_robot = Config().get_experiment(exp[\"robot_exp\"], exp[\"robot_run\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e716bce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the regression model\n",
    "\n",
    "model = VisProprio_SimpleMLPRegression(exp)\n",
    "model.to(device)\n",
    "if exp[\"loss\"] == \"MSE\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif exp[\"loss\"] == \"L1\":\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    raise Exception(f'Unknown loss type {exp[\"loss\"]}')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e52e51",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Create the training and validation data which maps latent encodings into robot position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388e394",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the original loading function\n",
    "\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_target_file\"])\n",
    "\n",
    "tr = load_demonstrations_as_proprioception_training(\n",
    "    sp, exp, spexp, exp_robot, \"training_data\", proprioception_input_file, proprioception_target_file, device=device\n",
    ")\n",
    "\n",
    "inputs_training = tr[\"inputs\"]\n",
    "targets_training = tr[\"targets\"]\n",
    "\n",
    "proprioception_test_input_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_test_input_file\"])\n",
    "proprioception_test_target_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_test_target_file\"])\n",
    "\n",
    "\n",
    "exp.start_timer(\"load-demos-as-proprioception-training\")\n",
    "tr_test = load_demonstrations_as_proprioception_training(\n",
    "    sp, exp, spexp, exp_robot, \"validation_data\", proprioception_test_input_file, proprioception_test_target_file, device=device\n",
    ")\n",
    "exp.end_timer(\"load-demos-as-proprioception-training\")\n",
    "\n",
    "inputs_validation = tr_test[\"inputs\"]\n",
    "targets_validation = tr_test[\"targets\"]\n",
    "\n",
    "# Create standard DataLoaders for single-view data\n",
    "batch_size = exp.get('batch_size', 32)\n",
    "# batch_size = exp['batch_size']\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c975f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a58de",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(exp):\n",
    "    \"\"\"Trains and saves the proprioception model, handling both single and multi-view inputs\n",
    "    with checkpoint support for resuming interrupted training\n",
    "    \"\"\"\n",
    "    final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "    checkpoint_dir = pathlib.Path(exp[\"data_dir\"], \"checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Maximum number of checkpoints to keep (excluding the best model)\n",
    "    max_checkpoints = 2\n",
    "\n",
    "    # Check if we're using a multi-view approach\n",
    "    is_multiview = exp.get(\"sensor_processing\", \"\").endswith(\"_multiview\") or exp.get(\"num_views\", 1) > 1\n",
    "    num_views = exp.get(\"num_views\", 2)\n",
    "\n",
    "    # First check for existing final model\n",
    "    if final_modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "        print(f\"Loading existing final model from {final_modelfile}\")\n",
    "        model.load_state_dict(torch.load(final_modelfile, map_location=device))\n",
    "\n",
    "        # Evaluate the loaded model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for batch_data in test_loader:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # Process the batch for evaluation\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                        sample_features = sp.process(sample_views)\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "            avg_loss = total_loss / max(batch_count, 1)\n",
    "            print(f\"Loaded model evaluation loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Function to extract epoch number from checkpoint file\n",
    "    def get_epoch_number(checkpoint_file):\n",
    "        try:\n",
    "            # Use a more robust approach to extract epoch number\n",
    "            # Format: epoch_XXXX.pth where XXXX is the epoch number\n",
    "            filename = checkpoint_file.stem\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                return int(parts[1])  # Get the number after \"epoch_\"\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    # Function to clean up old checkpoints\n",
    "    def cleanup_old_checkpoints():\n",
    "        # Get all epoch checkpoint files\n",
    "        checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "\n",
    "        # Sort by actual epoch number, not just filename\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        if len(checkpoint_files) > max_checkpoints:\n",
    "            files_to_delete = checkpoint_files[:-max_checkpoints]\n",
    "            for file_path in files_to_delete:\n",
    "                try:\n",
    "                    file_path.unlink()\n",
    "                    print(f\"Deleted old checkpoint: {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {file_path.name}: {e}\")\n",
    "\n",
    "    # Make sure model is on the correct device\n",
    "    model.to(device)\n",
    "    print(f\"Model moved to {device}\")\n",
    "\n",
    "    # Set training parameters\n",
    "    num_epochs = exp[\"epochs\"]\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # Check for existing checkpoints to resume from\n",
    "    checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number for more reliable ordering\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        # Get the most recent checkpoint\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        epoch_num = get_epoch_number(latest_checkpoint)\n",
    "\n",
    "        print(f\"Found checkpoint from epoch {epoch_num}. Resuming training...\")\n",
    "\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', float('inf'))\n",
    "\n",
    "        print(f\"Resuming from epoch {start_epoch}/{num_epochs} with best loss: {best_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Starting new training for {num_epochs} epochs\")\n",
    "\n",
    "    # Start or resume training\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Training loop handles both single and multi-view cases\n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            try:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # With multi-view, batch_views is a list of tensors, each with shape [batch_size, C, H, W]\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    # Process each sample in the batch\n",
    "                    for i in range(batch_size):\n",
    "                        # Extract this sample's views\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "\n",
    "                        # Process this sample through sp\n",
    "                        sample_features = sp.process(sample_views)\n",
    "\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    # Stack all samples' features into a batch\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    # Move to device\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    # Standard single-view processing\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "                # Print progress every few batches\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                # Save emergency checkpoint in case of error - use formatted epoch and batch numbers\n",
    "                save_path = checkpoint_dir / f\"emergency_epoch_{epoch:06d}_batch_{batch_idx:06d}.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_idx,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': total_loss / max(batch_count, 1),\n",
    "                    'best_loss': best_loss\n",
    "                }, save_path)\n",
    "                print(f\"Emergency checkpoint saved to {save_path}\")\n",
    "                continue\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = total_loss / max(batch_count, 1)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        eval_batch_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_data in test_loader:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # Process the batch the same way as in training\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                        sample_features = sp.process(sample_views)\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                test_loss += loss.item()\n",
    "                eval_batch_count += 1\n",
    "\n",
    "        avg_test_loss = test_loss / max(eval_batch_count, 1)\n",
    "        print(f'Validation Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "        # Save checkpoint after each epoch - using formatted epoch numbers for reliable sorting\n",
    "        checkpoint_path = checkpoint_dir / f\"epoch_{epoch:06d}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_loss,\n",
    "            'test_loss': avg_test_loss,\n",
    "            'best_loss': best_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        # Clean up old checkpoints to save space\n",
    "        cleanup_old_checkpoints()\n",
    "\n",
    "        # Update best model if improved\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_loss,\n",
    "                'test_loss': avg_test_loss,\n",
    "                'best_loss': best_loss\n",
    "            }, best_model_path)\n",
    "            print(f\"New best model saved with test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Training completed successfully\n",
    "    print(f\"Training complete. Best test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Load the best model for final save\n",
    "    best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "    if best_model_path.exists():\n",
    "        best_checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {best_checkpoint['epoch']+1} with test loss {best_checkpoint['test_loss']:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), final_modelfile)\n",
    "    print(f\"Final model saved to {final_modelfile}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554bf6e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\n",
    "\n",
    "#if modelfile.exists():\n",
    "#    model.load_state_dict(torch.load(modelfile))\n",
    "#else:\n",
    "exp.start_timer(\"train\")\n",
    "train_and_save_proprioception_model(exp)\n",
    "exp.end_timer(\"train\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dad443",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp[\"timer-load-demos-as-proprioception-training-end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3e038",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d03d26",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BerryPicker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.594537,
   "end_time": "2025-11-18T10:07:22.959895",
   "environment_variables": {},
   "exception": true,
   "input_path": "../visual_proprioception/Train_VisualProprioception.ipynb",
   "output_path": "/lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/visual_proprioception/~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result/Train_VisualProprioception_visual_proprioception_vp_conv_vae_256_0001_output.ipynb",
   "parameters": {
    "data_path": "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/result",
    "experiment": "visual_proprioception",
    "external_path": "~/WORK/BerryPicker-Flows/VisualProprioception_flow_01/exprun",
    "run": "vp_conv_vae_256_0001"
   },
   "start_time": "2025-11-18T10:07:17.365358",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}