{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models for visual proprioception\n",
    "\n",
    "Train a regression model for visual proprioception. The input is sensory data (eg. a camera image). This is encoded by a p;predefined sensorprocessing component into a latent representation. What we are training and saving here is a regressor that is mapping the latent representation to the position of the robot (eg. a vector of 6 degrees of freedom).\n",
    "\n",
    "The specification of this regressor is specified in an experiment of the type \"visual_proprioception\". Running this notebook will train and save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Loading pointer config file:\n",
      "\tC:\\Users\\lboloni\\.config\\BerryPicker\\mainsettings.yaml\n",
      "***ExpRun**: Loading machine-specific config file:\n",
      "\tG:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\settings-LotziYoga.yaml\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# from demonstration.demonstration import Demonstration\n",
    "\n",
    "\n",
    "from sensorprocessing.sp_factory import create_sp\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training, load_multiview_demonstrations_as_proprioception_training\n",
    "\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: No system dependent experiment file\n",
      "\t G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\visual_proprioception\\vp_convvae_128_sysdep.yaml,\n",
      "\t that is ok, proceeding.\n",
      "***ExpRun**: Configuration for exp/run: visual_proprioception/vp_convvae_128 successfully loaded\n",
      "Experiment:\n",
      "    batch_size: 64\n",
      "    data_dir: c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\n",
      "    encoding_size: 128\n",
      "    epochs: 1000\n",
      "    exp_run_sys_indep_file: C:\\Users\\lboloni\\Documents\\Code\\_Checkouts\\BerryPicker\\src\\experiment_configs\\visual_proprioception\\vp_convvae_128.yaml\n",
      "    experiment_name: visual_proprioception\n",
      "    loss: MSE\n",
      "    name: conv-vae-128\n",
      "    output_size: 6\n",
      "    proprioception_input_file: train_inputs.pt\n",
      "    proprioception_mlp_model_file: proprioception_mlp.pth\n",
      "    proprioception_target_file: train_targets.pt\n",
      "    proprioception_test_input_file: test_inputs.pt\n",
      "    proprioception_test_target_file: test_targets.pt\n",
      "    regressor_hidden_size_1: 64\n",
      "    regressor_hidden_size_2: 64\n",
      "    robot_exp: robot_al5d\n",
      "    robot_run: position_controller_00\n",
      "    run_name: vp_convvae_128\n",
      "    sensor_processing: ConvVaeSensorProcessing\n",
      "    sp_experiment: sensorprocessing_conv_vae\n",
      "    sp_run: sp_vae_128\n",
      "    subrun_name: null\n",
      "    training_data:\n",
      "    - - freeform\n",
      "      - '2024_12_26__16_40_20'\n",
      "      - dev2\n",
      "    - - freeform\n",
      "      - '2024_12_26__16_44_06'\n",
      "      - dev2\n",
      "    validation_data:\n",
      "    - - freeform\n",
      "      - '2024_12_26__16_40_20'\n",
      "      - dev2\n",
      "    - - freeform\n",
      "      - '2024_12_26__16_44_06'\n",
      "      - dev2\n",
      "\n",
      "***ExpRun**: No system dependent experiment file\n",
      "\t G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\sensorprocessing_conv_vae\\sp_vae_128_sysdep.yaml,\n",
      "\t that is ok, proceeding.\n",
      "***ExpRun**: Configuration for exp/run: sensorprocessing_conv_vae/sp_vae_128 successfully loaded\n",
      "Warning: logging configuration file is not found in logger\\logger_config.json.\n",
      "***ExpRun**: No system dependent experiment file\n",
      "\t G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\robot_al5d\\position_controller_00_sysdep.yaml,\n",
      "\t that is ok, proceeding.\n",
      "***ExpRun**: Configuration for exp/run: robot_al5d/position_controller_00 successfully loaded\n"
     ]
    }
   ],
   "source": [
    "experiment = \"visual_proprioception\"\n",
    "\n",
    "##############################################\n",
    "#                 SingleView                 #\n",
    "##############################################\n",
    "\n",
    "# the latent space 128 ones\n",
    "# run = \"vp_aruco_128\"\n",
    "# run = \"vp_convvae_128\"\n",
    "run = \"vp_convvae_128\"\n",
    "# run = \"vp_ptun_vgg19_128\"\n",
    "# run = \"vp_ptun_resnet50_128\"\n",
    "\n",
    "# the latent space 256 ones\n",
    "# run = \"vp_convvae_256\"\n",
    "# run = \"vp_ptun_vgg19_256\"\n",
    "# run = \"vp_ptun_resnet50_256\"\n",
    "\n",
    "#vits\n",
    "# run =\"vit_base\"\n",
    "# run =\"vit_large\"\n",
    "# run =\"vit_huge\"\n",
    "\n",
    "##############################################\n",
    "#                 MultiViews                 #\n",
    "##############################################\n",
    "\n",
    "#concat_proj\n",
    "\n",
    "# run =\"vit_base_multiview\"\n",
    "# run =\"vit_large_multiview\"\n",
    "# run =vit_huge_multiview\n",
    "\n",
    "\n",
    "##  indiv_proj\n",
    "# run = \"vit_base_multiview_indiv_proj\"  # ViT Base_indiv_proj\n",
    "# run = \"vit_large_multiview_indiv_proj\" # ViT Large_indiv_proj\n",
    "# run = \"vit_huge_multiview_indiv_proj\" # ViT Huge_indiv_proj\n",
    "\n",
    "##  attention\n",
    "# run = \"vit_base_multiview_attention\"  # ViT Base_attention\n",
    "# run = \"vit_large_multiview_attention\" # ViT Large_attention\n",
    "# run = \"vit_huge_multiview_attention\" # ViT Huge_attention\n",
    "\n",
    "\n",
    "##  weighted_sum\n",
    "# run = \"vit_base_multiview_weighted_sum\"  # ViT Base_weighted_sum\n",
    "# run = \"vit_large_multiview_weighted_sum\" # ViT Large_weighted_sum\n",
    "# run = \"vit_huge_multiview_weighted_sum\" # ViT Huge_weighted_sum\n",
    "\n",
    "##  gated\n",
    "# run = \"vit_base_multiview_gated\"  # ViT Base_gated\n",
    "# run = \"vit_large_multiview_gated\" # ViT Large_gated\n",
    "# run = \"vit_huge_multiview_gated\" # ViT Huge_gated\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)\n",
    "pprint(exp)\n",
    "\n",
    "# Create the sp object described in the experiment\n",
    "spexp = Config().get_experiment(exp[\"sp_experiment\"], exp[\"sp_run\"])\n",
    "sp = create_sp(spexp, device)\n",
    "exp_robot = Config().get_experiment(exp[\"robot_exp\"], exp[\"robot_run\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regression model\n",
    "\n",
    "model = VisProprio_SimpleMLPRegression(exp)\n",
    "model.to(device)\n",
    "if exp[\"loss\"] == \"MSE\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif exp[\"loss\"] == \"L1\":\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    raise Exception(f'Unknown loss type {exp[\"loss\"]}')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training and validation data which maps latent encodings into robot position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***load_demonstrations_as_proprioception_training*** \n",
      "\tSuccessfully loaded from cached files c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\train_inputs.pt etc\n",
      "***Timer*** load-demos-as-proprioception-training started\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_demonstrations_as_proprioception_training() missing 1 required positional argument: 'proprioception_target_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     17\u001b[0m proprioception_test_target_file \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\n\u001b[0;32m     18\u001b[0m     exp\u001b[38;5;241m.\u001b[39mdata_dir(), exp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproprioception_test_target_file\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     21\u001b[0m exp\u001b[38;5;241m.\u001b[39mstart_timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload-demos-as-proprioception-training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m tr_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_demonstrations_as_proprioception_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproprioception_test_input_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproprioception_test_target_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m exp\u001b[38;5;241m.\u001b[39mend_timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload-demos-as-proprioception-training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m inputs_validation \u001b[38;5;241m=\u001b[39m tr_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: load_demonstrations_as_proprioception_training() missing 1 required positional argument: 'proprioception_target_file'"
     ]
    }
   ],
   "source": [
    "# Use the original loading function\n",
    "\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_target_file\"])\n",
    "\n",
    "tr = load_demonstrations_as_proprioception_training(\n",
    "    sp, exp, spexp, exp_robot, \"training_data\", proprioception_input_file, proprioception_target_file, device=device\n",
    ")\n",
    "\n",
    "inputs_training = tr[\"inputs\"]\n",
    "targets_training = tr[\"targets\"]\n",
    "\n",
    "proprioception_test_input_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_test_input_file\"])\n",
    "proprioception_test_target_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_test_target_file\"])\n",
    "\n",
    "\n",
    "exp.start_timer(\"load-demos-as-proprioception-training\")\n",
    "tr_test = load_demonstrations_as_proprioception_training(\n",
    "    sp, exp, spexp, exp_robot, \"validation_data\", proprioception_test_input_file, proprioception_test_target_file, device=device\n",
    ")\n",
    "exp.end_timer(\"load-demos-as-proprioception-training\")\n",
    "\n",
    "inputs_validation = tr_test[\"inputs\"]\n",
    "targets_validation = tr_test[\"targets\"]\n",
    "\n",
    "# Create standard DataLoaders for single-view data\n",
    "# batch_size = exp.get('batch_size', 32)\n",
    "batch_size = exp['batch_size'] \n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(exp):\n",
    "    \"\"\"Trains and saves the proprioception model, handling both single and multi-view inputs\n",
    "    with checkpoint support for resuming interrupted training\n",
    "    \"\"\"\n",
    "    final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "    checkpoint_dir = pathlib.Path(exp[\"data_dir\"], \"checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Maximum number of checkpoints to keep (excluding the best model)\n",
    "    max_checkpoints = 2\n",
    "\n",
    "    # Check if we're using a multi-view approach\n",
    "    is_multiview = exp.get(\"sensor_processing\", \"\").endswith(\"_multiview\") or exp.get(\"num_views\", 1) > 1\n",
    "    num_views = exp.get(\"num_views\", 2)\n",
    "\n",
    "    # First check for existing final model\n",
    "    if final_modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "        print(f\"Loading existing final model from {final_modelfile}\")\n",
    "        model.load_state_dict(torch.load(final_modelfile, map_location=device))\n",
    "\n",
    "        # Evaluate the loaded model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for batch_data in test_loader:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # Process the batch for evaluation\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                        sample_features = sp.process(sample_views)\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "            avg_loss = total_loss / max(batch_count, 1)\n",
    "            print(f\"Loaded model evaluation loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Function to extract epoch number from checkpoint file\n",
    "    def get_epoch_number(checkpoint_file):\n",
    "        try:\n",
    "            # Use a more robust approach to extract epoch number\n",
    "            # Format: epoch_XXXX.pth where XXXX is the epoch number\n",
    "            filename = checkpoint_file.stem\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                return int(parts[1])  # Get the number after \"epoch_\"\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    # Function to clean up old checkpoints\n",
    "    def cleanup_old_checkpoints():\n",
    "        # Get all epoch checkpoint files\n",
    "        checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "\n",
    "        # Sort by actual epoch number, not just filename\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        if len(checkpoint_files) > max_checkpoints:\n",
    "            files_to_delete = checkpoint_files[:-max_checkpoints]\n",
    "            for file_path in files_to_delete:\n",
    "                try:\n",
    "                    file_path.unlink()\n",
    "                    print(f\"Deleted old checkpoint: {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {file_path.name}: {e}\")\n",
    "\n",
    "    # Make sure model is on the correct device\n",
    "    model.to(device)\n",
    "    print(f\"Model moved to {device}\")\n",
    "\n",
    "    # Set training parameters\n",
    "    num_epochs = exp[\"epochs\"]\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # Check for existing checkpoints to resume from\n",
    "    checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number for more reliable ordering\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        # Get the most recent checkpoint\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        epoch_num = get_epoch_number(latest_checkpoint)\n",
    "\n",
    "        print(f\"Found checkpoint from epoch {epoch_num}. Resuming training...\")\n",
    "\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', float('inf'))\n",
    "\n",
    "        print(f\"Resuming from epoch {start_epoch}/{num_epochs} with best loss: {best_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Starting new training for {num_epochs} epochs\")\n",
    "\n",
    "    # Start or resume training\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Training loop handles both single and multi-view cases\n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            try:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # With multi-view, batch_views is a list of tensors, each with shape [batch_size, C, H, W]\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    # Process each sample in the batch\n",
    "                    for i in range(batch_size):\n",
    "                        # Extract this sample's views\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "\n",
    "                        # Process this sample through sp\n",
    "                        sample_features = sp.process(sample_views)\n",
    "\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    # Stack all samples' features into a batch\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    # Move to device\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    # Standard single-view processing\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "                # Print progress every few batches\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                # Save emergency checkpoint in case of error - use formatted epoch and batch numbers\n",
    "                save_path = checkpoint_dir / f\"emergency_epoch_{epoch:06d}_batch_{batch_idx:06d}.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_idx,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': total_loss / max(batch_count, 1),\n",
    "                    'best_loss': best_loss\n",
    "                }, save_path)\n",
    "                print(f\"Emergency checkpoint saved to {save_path}\")\n",
    "                continue\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = total_loss / max(batch_count, 1)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        eval_batch_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_data in test_loader:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # Process the batch the same way as in training\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                        sample_features = sp.process(sample_views)\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                test_loss += loss.item()\n",
    "                eval_batch_count += 1\n",
    "\n",
    "        avg_test_loss = test_loss / max(eval_batch_count, 1)\n",
    "        print(f'Validation Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "        # Save checkpoint after each epoch - using formatted epoch numbers for reliable sorting\n",
    "        checkpoint_path = checkpoint_dir / f\"epoch_{epoch:06d}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_loss,\n",
    "            'test_loss': avg_test_loss,\n",
    "            'best_loss': best_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        # Clean up old checkpoints to save space\n",
    "        cleanup_old_checkpoints()\n",
    "\n",
    "        # Update best model if improved\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_loss,\n",
    "                'test_loss': avg_test_loss,\n",
    "                'best_loss': best_loss\n",
    "            }, best_model_path)\n",
    "            print(f\"New best model saved with test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Training completed successfully\n",
    "    print(f\"Training complete. Best test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Load the best model for final save\n",
    "    best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "    if best_model_path.exists():\n",
    "        best_checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {best_checkpoint['epoch']+1} with test loss {best_checkpoint['test_loss']:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), final_modelfile)\n",
    "    print(f\"Final model saved to {final_modelfile}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\n",
    "\n",
    "#if modelfile.exists():\n",
    "#    model.load_state_dict(torch.load(modelfile))\n",
    "#else:\n",
    "exp.start_timer(\"train\")\n",
    "train_and_save_proprioception_model(exp)\n",
    "exp.end_timer(\"train\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp[\"timer-load-demos-as-proprioception-training-end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
