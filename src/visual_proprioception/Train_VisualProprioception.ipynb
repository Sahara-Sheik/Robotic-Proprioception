{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models for visual proprioception\n",
    "\n",
    "Train a regression model for visual proprioception. The input is sensory data (eg. a camera image). This is encoded by a p;predefined sensorprocessing component into a latent representation. What we are training and saving here is a regressor that is mapping the latent representation to the position of the robot (eg. a vector of 6 degrees of freedom).\n",
    "\n",
    "The specification of this regressor is specified in an experiment of the type \"visual_proprioception\". Running this notebook will train and save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from demonstration.demonstration import Demonstration\n",
    "\n",
    "import sensorprocessing.sp_helper as sp_helper\n",
    "\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training_old, get_visual_proprioception_sp, load_multiview_demonstrations_as_proprioception_training\n",
    "\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# just for testing\n",
    "# device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"visual_proprioception\"\n",
    "\n",
    "##############################################\n",
    "#                 SingleView                 #\n",
    "##############################################\n",
    "\n",
    "# the latent space 128 ones\n",
    "# run = \"vp_aruco_128\"\n",
    "run = \"vp_convvae_128\"\n",
    "# run = \"vp_ptun_vgg19_128\"\n",
    "# run = \"vp_ptun_resnet50_128\"\n",
    "\n",
    "# the latent space 256 ones\n",
    "# run = \"vp_convvae_256\"\n",
    "# run = \"vp_ptun_vgg19_256\"\n",
    "# run = \"vp_ptun_resnet50_256\"\n",
    "\n",
    "#vits\n",
    "# run =\"vit_base\"\n",
    "# run =\"vit_large\"\n",
    "# run =\"vit_huge\"\n",
    "\n",
    "##############################################\n",
    "#                 MultiViews                 #\n",
    "##############################################\n",
    "\n",
    "#concat_proj\n",
    "\n",
    "# run =\"vit_base_multiview\"\n",
    "# run =\"vit_large_multiview\"\n",
    "# run =vit_huge_multiview\n",
    "\n",
    "\n",
    "##  indiv_proj\n",
    "# run = \"vit_base_multiview_indiv_proj\"  # ViT Base_indiv_proj\n",
    "# run = \"vit_large_multiview_indiv_proj\" # ViT Large_indiv_proj\n",
    "# run = \"vit_huge_multiview_indiv_proj\" # ViT Huge_indiv_proj\n",
    "\n",
    "##  attention\n",
    "# run = \"vit_base_multiview_attention\"  # ViT Base_attention\n",
    "# run = \"vit_large_multiview_attention\" # ViT Large_attention\n",
    "# run = \"vit_huge_multiview_attention\" # ViT Huge_attention\n",
    "\n",
    "\n",
    "##  weighted_sum\n",
    "# run = \"vit_base_multiview_weighted_sum\"  # ViT Base_weighted_sum\n",
    "# run = \"vit_large_multiview_weighted_sum\" # ViT Large_weighted_sum\n",
    "# run = \"vit_huge_multiview_weighted_sum\" # ViT Huge_weighted_sum\n",
    "\n",
    "##  gated\n",
    "# run = \"vit_base_multiview_gated\"  # ViT Base_gated\n",
    "# run = \"vit_large_multiview_gated\" # ViT Large_gated\n",
    "# run = \"vit_huge_multiview_gated\" # ViT Huge_gated\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)\n",
    "pprint(exp)\n",
    "\n",
    "# Create the sp object described in the experiment\n",
    "spexp = Config().get_experiment(exp[\"sp_experiment\"], exp[\"sp_run\"])\n",
    "sp = get_visual_proprioception_sp(exp, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regression model\n",
    "\n",
    "model = VisProprio_SimpleMLPRegression(exp)\n",
    "if exp[\"loss\"] == \"MSE\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif exp[\"loss\"] == \"L1\":\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    raise Exception(f'Unknown loss type {exp[\"loss\"]}')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and cache the training data. \n",
    "* Iterate through the images and process them into latent encodings. \n",
    "* Iterate through the json files describing the robot position\n",
    "* Save the input and target values into files in the experiment directory. These will act as caches for later runs\n",
    "* Create the training and validation splits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_target_file\"])\n",
    "\n",
    "\n",
    "# Check if we're using a multi-view approach\n",
    "is_multiview = exp.get(\"sensor_processing\", \"\").endswith(\"_multiview\") or exp.get(\"num_views\", 1) > 1\n",
    "\n",
    "if is_multiview:\n",
    "    print(f\"Using multi-view approach with {exp.get('num_views', 2)} views\")\n",
    "\n",
    "    # Use the multiview loading function\n",
    "    tr = load_multiview_demonstrations_as_proprioception_training(\n",
    "        task,\n",
    "        proprioception_input_file,\n",
    "        proprioception_target_file,\n",
    "        num_views=exp.get(\"num_views\", 2)\n",
    "    )\n",
    "\n",
    "    # Create a custom dataset for multi-view data\n",
    "    class MultiViewDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, view_inputs, targets):\n",
    "            self.view_inputs = view_inputs  # List of tensors, one per view\n",
    "            self.targets = targets\n",
    "            self.num_samples = len(targets)\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # Get corresponding sample from each view\n",
    "            views = [view[idx] for view in self.view_inputs]\n",
    "            target = self.targets[idx]\n",
    "            return views, target\n",
    "\n",
    "    # Create DataLoaders for batching\n",
    "    batch_size = exp.get('batch_size', 32)\n",
    "    train_dataset = MultiViewDataset(tr[\"view_inputs_training\"], tr[\"targets_training\"])\n",
    "    test_dataset = MultiViewDataset(tr[\"view_inputs_validation\"], tr[\"targets_validation\"])\n",
    "\n",
    "else:\n",
    "    print(\"Using single-view approach\")\n",
    "\n",
    "    # Use the original loading function\n",
    "    tr = load_demonstrations_as_proprioception_training(\n",
    "        sp, task, proprioception_input_file, proprioception_target_file\n",
    "    )\n",
    "\n",
    "    inputs_training = tr[\"inputs_training\"]\n",
    "    targets_training = tr[\"targets_training\"]\n",
    "    inputs_validation = tr[\"inputs_validation\"]\n",
    "    targets_validation = tr[\"targets_validation\"]\n",
    "\n",
    "    # Create standard DataLoaders for single-view data\n",
    "    batch_size = exp.get('batch_size', 32)\n",
    "    train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "    test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "def load_demonstrations_as_proprioception_training(sp, exp, spexp, datasetname, proprioception_input_file, proprioception_target_file, device=None):\n",
    "    \"\"\"Loads all the images from the specified dataset and creates the input and target tensors. \"\"\"\n",
    "    inputlist = []\n",
    "    targetlist = []\n",
    "    transform = sp_helper.get_transform_to_sp(spexp)\n",
    "    \n",
    "    for val in exp[datasetname]:\n",
    "        run, demo_name, camera = val\n",
    "        exp_demo = Config().get_experiment(\"demonstration\", run)\n",
    "        demo = Demonstration(exp_demo, demo_name)\n",
    "        for i in range(demo.metadata[\"maxsteps\"]):\n",
    "            sensor_readings, _ = demo.get_image(i, camera=camera, transform=transform, device=device)\n",
    "            z = sp.process(sensor_readings)\n",
    "            a = demo.get_action(i)\n",
    "            #anorm = np.zeros(a.shape, np.float32)\n",
    "            rp = RobotPosition.from_vector(a)\n",
    "            anorm = rp.to_normalized_vector()\n",
    "            inp = torch.from_numpy(z)\n",
    "            tgt = torch.from_numpy(anorm)\n",
    "            inputlist.append(inp)\n",
    "            targetlist.append(tgt)\n",
    "    retval = {}\n",
    "    retval[\"inputs\"] = torch.stack(inputlist)\n",
    "    retval[\"targets\"] = torch.stack(targetlist)\n",
    "    torch.save(retval[\"inputs\"], proprioception_input_file)\n",
    "    torch.save(retval[\"targets\"], proprioception_target_file)\n",
    "    return retval            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the original loading function\n",
    "\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_target_file\"])\n",
    "\n",
    "tr = load_demonstrations_as_proprioception_training(\n",
    "    sp, exp, spexp, \"training_data\", proprioception_input_file, proprioception_target_file, device=device\n",
    ")\n",
    "\n",
    "inputs_training = tr[\"inputs\"]\n",
    "targets_training = tr[\"targets\"]\n",
    "\n",
    "proprioception_test_input_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_test_input_file\"])\n",
    "proprioception_test_target_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_test_target_file\"])\n",
    "\n",
    "\n",
    "tr_test = load_demonstrations_as_proprioception_training(\n",
    "    sp, exp, spexp, \"validation_data\", proprioception_test_input_file, proprioception_test_target_file, device=device\n",
    ")\n",
    "\n",
    "inputs_validation = tr_test[\"inputs\"]\n",
    "targets_validation = tr_test[\"targets\"]\n",
    "\n",
    "# Create standard DataLoaders for single-view data\n",
    "# batch_size = exp.get('batch_size', 32)\n",
    "batch_size = exp['batch_size'] \n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(exp):\n",
    "    \"\"\"Trains and saves the proprioception model, handling both single and multi-view inputs\n",
    "    with checkpoint support for resuming interrupted training\n",
    "    \"\"\"\n",
    "    final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "    checkpoint_dir = pathlib.Path(exp[\"data_dir\"], \"checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Maximum number of checkpoints to keep (excluding the best model)\n",
    "    max_checkpoints = 2\n",
    "\n",
    "    # Check if we're using a multi-view approach\n",
    "    is_multiview = exp.get(\"sensor_processing\", \"\").endswith(\"_multiview\") or exp.get(\"num_views\", 1) > 1\n",
    "    num_views = exp.get(\"num_views\", 2)\n",
    "\n",
    "    # First check for existing final model\n",
    "    if final_modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "        print(f\"Loading existing final model from {final_modelfile}\")\n",
    "        model.load_state_dict(torch.load(final_modelfile, map_location=device))\n",
    "\n",
    "        # Evaluate the loaded model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for batch_data in test_loader:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # Process the batch for evaluation\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                        sample_features = sp.process(sample_views)\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "            avg_loss = total_loss / max(batch_count, 1)\n",
    "            print(f\"Loaded model evaluation loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Function to extract epoch number from checkpoint file\n",
    "    def get_epoch_number(checkpoint_file):\n",
    "        try:\n",
    "            # Use a more robust approach to extract epoch number\n",
    "            # Format: epoch_XXXX.pth where XXXX is the epoch number\n",
    "            filename = checkpoint_file.stem\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                return int(parts[1])  # Get the number after \"epoch_\"\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    # Function to clean up old checkpoints\n",
    "    def cleanup_old_checkpoints():\n",
    "        # Get all epoch checkpoint files\n",
    "        checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "\n",
    "        # Sort by actual epoch number, not just filename\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        if len(checkpoint_files) > max_checkpoints:\n",
    "            files_to_delete = checkpoint_files[:-max_checkpoints]\n",
    "            for file_path in files_to_delete:\n",
    "                try:\n",
    "                    file_path.unlink()\n",
    "                    print(f\"Deleted old checkpoint: {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {file_path.name}: {e}\")\n",
    "\n",
    "    # Make sure model is on the correct device\n",
    "    model.to(device)\n",
    "    print(f\"Model moved to {device}\")\n",
    "\n",
    "    # Set training parameters\n",
    "    num_epochs = exp[\"epochs\"]\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # Check for existing checkpoints to resume from\n",
    "    checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number for more reliable ordering\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        # Get the most recent checkpoint\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        epoch_num = get_epoch_number(latest_checkpoint)\n",
    "\n",
    "        print(f\"Found checkpoint from epoch {epoch_num}. Resuming training...\")\n",
    "\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', float('inf'))\n",
    "\n",
    "        print(f\"Resuming from epoch {start_epoch}/{num_epochs} with best loss: {best_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Starting new training for {num_epochs} epochs\")\n",
    "\n",
    "    # Start or resume training\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Training loop handles both single and multi-view cases\n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            try:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # With multi-view, batch_views is a list of tensors, each with shape [batch_size, C, H, W]\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    # Process each sample in the batch\n",
    "                    for i in range(batch_size):\n",
    "                        # Extract this sample's views\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "\n",
    "                        # Process this sample through sp\n",
    "                        sample_features = sp.process(sample_views)\n",
    "\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    # Stack all samples' features into a batch\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    # Move to device\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    # Standard single-view processing\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "                # Print progress every few batches\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                # Save emergency checkpoint in case of error - use formatted epoch and batch numbers\n",
    "                save_path = checkpoint_dir / f\"emergency_epoch_{epoch:06d}_batch_{batch_idx:06d}.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_idx,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': total_loss / max(batch_count, 1),\n",
    "                    'best_loss': best_loss\n",
    "                }, save_path)\n",
    "                print(f\"Emergency checkpoint saved to {save_path}\")\n",
    "                continue\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = total_loss / max(batch_count, 1)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        eval_batch_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_data in test_loader:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # Process the batch the same way as in training\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                        sample_features = sp.process(sample_views)\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                test_loss += loss.item()\n",
    "                eval_batch_count += 1\n",
    "\n",
    "        avg_test_loss = test_loss / max(eval_batch_count, 1)\n",
    "        print(f'Validation Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "        # Save checkpoint after each epoch - using formatted epoch numbers for reliable sorting\n",
    "        checkpoint_path = checkpoint_dir / f\"epoch_{epoch:06d}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_loss,\n",
    "            'test_loss': avg_test_loss,\n",
    "            'best_loss': best_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        # Clean up old checkpoints to save space\n",
    "        cleanup_old_checkpoints()\n",
    "\n",
    "        # Update best model if improved\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_loss,\n",
    "                'test_loss': avg_test_loss,\n",
    "                'best_loss': best_loss\n",
    "            }, best_model_path)\n",
    "            print(f\"New best model saved with test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Training completed successfully\n",
    "    print(f\"Training complete. Best test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Load the best model for final save\n",
    "    best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "    if best_model_path.exists():\n",
    "        best_checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {best_checkpoint['epoch']+1} with test loss {best_checkpoint['test_loss']:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), final_modelfile)\n",
    "    print(f\"Final model saved to {final_modelfile}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/31, Loss: 1.0417\n",
      "  Batch 20/31, Loss: 1.1085\n",
      "  Batch 30/31, Loss: 0.6811\n",
      "Epoch [166/1000], Loss: 0.7582\n",
      "Validation Loss: 0.8286\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000165.pth\n",
      "Deleted old checkpoint: epoch_000163.pth\n",
      "Starting epoch 167/1000\n",
      "  Batch 10/31, Loss: 0.8239\n",
      "  Batch 20/31, Loss: 0.8018\n",
      "  Batch 30/31, Loss: 1.1086\n",
      "Epoch [167/1000], Loss: 0.7370\n",
      "Validation Loss: 0.7525\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000166.pth\n",
      "Deleted old checkpoint: epoch_000164.pth\n",
      "Starting epoch 168/1000\n",
      "  Batch 10/31, Loss: 1.1125\n",
      "  Batch 20/31, Loss: 1.0207\n",
      "  Batch 30/31, Loss: 0.3324\n",
      "Epoch [168/1000], Loss: 0.8053\n",
      "Validation Loss: 0.7778\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000167.pth\n",
      "Deleted old checkpoint: epoch_000165.pth\n",
      "Starting epoch 169/1000\n",
      "  Batch 10/31, Loss: 0.5770\n",
      "  Batch 20/31, Loss: 0.8753\n",
      "  Batch 30/31, Loss: 0.6237\n",
      "Epoch [169/1000], Loss: 0.7416\n",
      "Validation Loss: 0.7572\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000168.pth\n",
      "Deleted old checkpoint: epoch_000166.pth\n",
      "Starting epoch 170/1000\n",
      "  Batch 10/31, Loss: 0.8559\n",
      "  Batch 20/31, Loss: 0.7610\n",
      "  Batch 30/31, Loss: 0.5900\n",
      "Epoch [170/1000], Loss: 0.7748\n",
      "Validation Loss: 0.7045\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000169.pth\n",
      "Deleted old checkpoint: epoch_000167.pth\n",
      "Starting epoch 171/1000\n",
      "  Batch 10/31, Loss: 0.7772\n",
      "  Batch 20/31, Loss: 0.6610\n",
      "  Batch 30/31, Loss: 0.5800\n",
      "Epoch [171/1000], Loss: 0.7415\n",
      "Validation Loss: 0.7058\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000170.pth\n",
      "Deleted old checkpoint: epoch_000168.pth\n",
      "Starting epoch 172/1000\n",
      "  Batch 10/31, Loss: 0.9969\n",
      "  Batch 20/31, Loss: 0.7438\n",
      "  Batch 30/31, Loss: 0.9612\n",
      "Epoch [172/1000], Loss: 0.7510\n",
      "Validation Loss: 0.7049\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000171.pth\n",
      "Deleted old checkpoint: epoch_000169.pth\n",
      "Starting epoch 173/1000\n",
      "  Batch 10/31, Loss: 0.6183\n",
      "  Batch 20/31, Loss: 0.7472\n",
      "  Batch 30/31, Loss: 0.4406\n",
      "Epoch [173/1000], Loss: 0.7202\n",
      "Validation Loss: 0.7019\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000172.pth\n",
      "Deleted old checkpoint: epoch_000170.pth\n",
      "Starting epoch 174/1000\n",
      "  Batch 10/31, Loss: 0.7034\n",
      "  Batch 20/31, Loss: 0.4283\n",
      "  Batch 30/31, Loss: 0.6467\n",
      "Epoch [174/1000], Loss: 0.7268\n",
      "Validation Loss: 0.7653\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000173.pth\n",
      "Deleted old checkpoint: epoch_000171.pth\n",
      "Starting epoch 175/1000\n",
      "  Batch 10/31, Loss: 0.3956\n",
      "  Batch 20/31, Loss: 0.5912\n",
      "  Batch 30/31, Loss: 0.8329\n",
      "Epoch [175/1000], Loss: 0.7252\n",
      "Validation Loss: 0.7171\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000174.pth\n",
      "Deleted old checkpoint: epoch_000172.pth\n",
      "Starting epoch 176/1000\n",
      "  Batch 10/31, Loss: 0.4513\n",
      "  Batch 20/31, Loss: 0.4348\n",
      "  Batch 30/31, Loss: 0.9343\n",
      "Epoch [176/1000], Loss: 0.7078\n",
      "Validation Loss: 0.6742\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000175.pth\n",
      "Deleted old checkpoint: epoch_000173.pth\n",
      "New best model saved with test loss: 0.6742\n",
      "Starting epoch 177/1000\n",
      "  Batch 10/31, Loss: 0.5984\n",
      "  Batch 20/31, Loss: 0.5664\n",
      "  Batch 30/31, Loss: 0.7833\n",
      "Epoch [177/1000], Loss: 0.7166\n",
      "Validation Loss: 0.7132\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000176.pth\n",
      "Deleted old checkpoint: epoch_000174.pth\n",
      "Starting epoch 178/1000\n",
      "  Batch 10/31, Loss: 0.7460\n",
      "  Batch 20/31, Loss: 0.9683\n",
      "  Batch 30/31, Loss: 0.7954\n",
      "Epoch [178/1000], Loss: 0.7005\n",
      "Validation Loss: 0.6984\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000177.pth\n",
      "Deleted old checkpoint: epoch_000175.pth\n",
      "Starting epoch 179/1000\n",
      "  Batch 10/31, Loss: 0.7464\n",
      "  Batch 20/31, Loss: 0.8987\n",
      "  Batch 30/31, Loss: 0.7964\n",
      "Epoch [179/1000], Loss: 0.6964\n",
      "Validation Loss: 0.6728\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000178.pth\n",
      "Deleted old checkpoint: epoch_000176.pth\n",
      "New best model saved with test loss: 0.6728\n",
      "Starting epoch 180/1000\n",
      "  Batch 10/31, Loss: 0.4111\n",
      "  Batch 20/31, Loss: 0.3916\n",
      "  Batch 30/31, Loss: 1.1091\n",
      "Epoch [180/1000], Loss: 0.7430\n",
      "Validation Loss: 0.7102\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000179.pth\n",
      "Deleted old checkpoint: epoch_000177.pth\n",
      "Starting epoch 181/1000\n",
      "  Batch 10/31, Loss: 1.1850\n",
      "  Batch 20/31, Loss: 0.7180\n",
      "  Batch 30/31, Loss: 0.5429\n",
      "Epoch [181/1000], Loss: 0.7541\n",
      "Validation Loss: 0.6649\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000180.pth\n",
      "Deleted old checkpoint: epoch_000178.pth\n",
      "New best model saved with test loss: 0.6649\n",
      "Starting epoch 182/1000\n",
      "  Batch 10/31, Loss: 0.7353\n",
      "  Batch 20/31, Loss: 0.8113\n",
      "  Batch 30/31, Loss: 0.7591\n",
      "Epoch [182/1000], Loss: 0.7115\n",
      "Validation Loss: 0.7667\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000181.pth\n",
      "Deleted old checkpoint: epoch_000179.pth\n",
      "Starting epoch 183/1000\n",
      "  Batch 10/31, Loss: 0.6846\n",
      "  Batch 20/31, Loss: 0.4487\n",
      "  Batch 30/31, Loss: 0.4447\n",
      "Epoch [183/1000], Loss: 0.6872\n",
      "Validation Loss: 0.7324\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000182.pth\n",
      "Deleted old checkpoint: epoch_000180.pth\n",
      "Starting epoch 184/1000\n",
      "  Batch 10/31, Loss: 0.5187\n",
      "  Batch 20/31, Loss: 0.9438\n",
      "  Batch 30/31, Loss: 0.6976\n",
      "Epoch [184/1000], Loss: 0.7466\n",
      "Validation Loss: 0.6546\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000183.pth\n",
      "Deleted old checkpoint: epoch_000181.pth\n",
      "New best model saved with test loss: 0.6546\n",
      "Starting epoch 185/1000\n",
      "  Batch 10/31, Loss: 0.3242\n",
      "  Batch 20/31, Loss: 0.7704\n",
      "  Batch 30/31, Loss: 0.4285\n",
      "Epoch [185/1000], Loss: 0.7009\n",
      "Validation Loss: 0.7257\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000184.pth\n",
      "Deleted old checkpoint: epoch_000182.pth\n",
      "Starting epoch 186/1000\n",
      "  Batch 10/31, Loss: 0.9294\n",
      "  Batch 20/31, Loss: 0.8296\n",
      "  Batch 30/31, Loss: 0.7946\n",
      "Epoch [186/1000], Loss: 0.7056\n",
      "Validation Loss: 0.6976\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000185.pth\n",
      "Deleted old checkpoint: epoch_000183.pth\n",
      "Starting epoch 187/1000\n",
      "  Batch 10/31, Loss: 0.4394\n",
      "  Batch 20/31, Loss: 0.5061\n",
      "  Batch 30/31, Loss: 0.8998\n",
      "Epoch [187/1000], Loss: 0.7461\n",
      "Validation Loss: 0.8148\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000186.pth\n",
      "Deleted old checkpoint: epoch_000184.pth\n",
      "Starting epoch 188/1000\n",
      "  Batch 10/31, Loss: 0.4323\n",
      "  Batch 20/31, Loss: 0.6487\n",
      "  Batch 30/31, Loss: 0.9879\n",
      "Epoch [188/1000], Loss: 0.7392\n",
      "Validation Loss: 0.7207\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000187.pth\n",
      "Deleted old checkpoint: epoch_000185.pth\n",
      "Starting epoch 189/1000\n",
      "  Batch 10/31, Loss: 1.1568\n",
      "  Batch 20/31, Loss: 0.6459\n",
      "  Batch 30/31, Loss: 0.3867\n",
      "Epoch [189/1000], Loss: 0.7290\n",
      "Validation Loss: 0.6741\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000188.pth\n",
      "Deleted old checkpoint: epoch_000186.pth\n",
      "Starting epoch 190/1000\n",
      "  Batch 10/31, Loss: 0.4849\n",
      "  Batch 20/31, Loss: 0.7758\n",
      "  Batch 30/31, Loss: 0.7604\n",
      "Epoch [190/1000], Loss: 0.7200\n",
      "Validation Loss: 0.6800\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000189.pth\n",
      "Deleted old checkpoint: epoch_000187.pth\n",
      "Starting epoch 191/1000\n",
      "  Batch 10/31, Loss: 0.6806\n",
      "  Batch 20/31, Loss: 0.7452\n",
      "  Batch 30/31, Loss: 0.7729\n",
      "Epoch [191/1000], Loss: 0.7701\n",
      "Validation Loss: 0.6702\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000190.pth\n",
      "Deleted old checkpoint: epoch_000188.pth\n",
      "Starting epoch 192/1000\n",
      "  Batch 10/31, Loss: 1.0131\n",
      "  Batch 20/31, Loss: 0.4846\n",
      "  Batch 30/31, Loss: 0.8887\n",
      "Epoch [192/1000], Loss: 0.7527\n",
      "Validation Loss: 0.6537\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000191.pth\n",
      "Deleted old checkpoint: epoch_000189.pth\n",
      "New best model saved with test loss: 0.6537\n",
      "Starting epoch 193/1000\n",
      "  Batch 10/31, Loss: 0.7570\n",
      "  Batch 20/31, Loss: 0.3243\n",
      "  Batch 30/31, Loss: 0.5989\n",
      "Epoch [193/1000], Loss: 0.6640\n",
      "Validation Loss: 0.6877\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000192.pth\n",
      "Deleted old checkpoint: epoch_000190.pth\n",
      "Starting epoch 194/1000\n",
      "  Batch 10/31, Loss: 0.6981\n",
      "  Batch 20/31, Loss: 1.0727\n",
      "  Batch 30/31, Loss: 0.7798\n",
      "Epoch [194/1000], Loss: 0.6880\n",
      "Validation Loss: 0.6406\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000193.pth\n",
      "Deleted old checkpoint: epoch_000191.pth\n",
      "New best model saved with test loss: 0.6406\n",
      "Starting epoch 195/1000\n",
      "  Batch 10/31, Loss: 0.7174\n",
      "  Batch 20/31, Loss: 0.4384\n",
      "  Batch 30/31, Loss: 0.5763\n",
      "Epoch [195/1000], Loss: 0.6620\n",
      "Validation Loss: 0.6315\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000194.pth\n",
      "Deleted old checkpoint: epoch_000192.pth\n",
      "New best model saved with test loss: 0.6315\n",
      "Starting epoch 196/1000\n",
      "  Batch 10/31, Loss: 0.7112\n",
      "  Batch 20/31, Loss: 0.9619\n",
      "  Batch 30/31, Loss: 0.7023\n",
      "Epoch [196/1000], Loss: 0.6679\n",
      "Validation Loss: 0.6328\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000195.pth\n",
      "Deleted old checkpoint: epoch_000193.pth\n",
      "Starting epoch 197/1000\n",
      "  Batch 10/31, Loss: 0.4044\n",
      "  Batch 20/31, Loss: 0.5919\n",
      "  Batch 30/31, Loss: 0.7215\n",
      "Epoch [197/1000], Loss: 0.7278\n",
      "Validation Loss: 0.8078\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000196.pth\n",
      "Deleted old checkpoint: epoch_000194.pth\n",
      "Starting epoch 198/1000\n",
      "  Batch 10/31, Loss: 0.9182\n",
      "  Batch 20/31, Loss: 0.5889\n",
      "  Batch 30/31, Loss: 0.5767\n",
      "Epoch [198/1000], Loss: 0.7611\n",
      "Validation Loss: 0.6775\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000197.pth\n",
      "Deleted old checkpoint: epoch_000195.pth\n",
      "Starting epoch 199/1000\n",
      "  Batch 10/31, Loss: 0.4678\n",
      "  Batch 20/31, Loss: 0.5503\n",
      "  Batch 30/31, Loss: 0.5591\n",
      "Epoch [199/1000], Loss: 0.6792\n",
      "Validation Loss: 0.6125\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000198.pth\n",
      "Deleted old checkpoint: epoch_000196.pth\n",
      "New best model saved with test loss: 0.6125\n",
      "Starting epoch 200/1000\n",
      "  Batch 10/31, Loss: 0.4428\n",
      "  Batch 20/31, Loss: 0.8926\n",
      "  Batch 30/31, Loss: 0.6439\n",
      "Epoch [200/1000], Loss: 0.6679\n",
      "Validation Loss: 0.6761\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000199.pth\n",
      "Deleted old checkpoint: epoch_000197.pth\n",
      "Starting epoch 201/1000\n",
      "  Batch 10/31, Loss: 0.5861\n",
      "  Batch 20/31, Loss: 0.9866\n",
      "  Batch 30/31, Loss: 1.1243\n",
      "Epoch [201/1000], Loss: 0.6431\n",
      "Validation Loss: 0.6199\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000200.pth\n",
      "Deleted old checkpoint: epoch_000198.pth\n",
      "Starting epoch 202/1000\n",
      "  Batch 10/31, Loss: 0.6498\n",
      "  Batch 20/31, Loss: 0.3548\n",
      "  Batch 30/31, Loss: 0.6689\n",
      "Epoch [202/1000], Loss: 0.6254\n",
      "Validation Loss: 0.6368\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000201.pth\n",
      "Deleted old checkpoint: epoch_000199.pth\n",
      "Starting epoch 203/1000\n",
      "  Batch 10/31, Loss: 0.5566\n",
      "  Batch 20/31, Loss: 0.4723\n",
      "  Batch 30/31, Loss: 0.6027\n",
      "Epoch [203/1000], Loss: 0.6441\n",
      "Validation Loss: 0.6771\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000202.pth\n",
      "Deleted old checkpoint: epoch_000200.pth\n",
      "Starting epoch 204/1000\n",
      "  Batch 10/31, Loss: 0.5484\n",
      "  Batch 20/31, Loss: 0.9034\n",
      "  Batch 30/31, Loss: 0.5469\n",
      "Epoch [204/1000], Loss: 0.6716\n",
      "Validation Loss: 0.6564\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000203.pth\n",
      "Deleted old checkpoint: epoch_000201.pth\n",
      "Starting epoch 205/1000\n",
      "  Batch 10/31, Loss: 0.6414\n",
      "  Batch 20/31, Loss: 0.9067\n",
      "  Batch 30/31, Loss: 0.7841\n",
      "Epoch [205/1000], Loss: 0.6659\n",
      "Validation Loss: 0.6130\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000204.pth\n",
      "Deleted old checkpoint: epoch_000202.pth\n",
      "Starting epoch 206/1000\n",
      "  Batch 10/31, Loss: 0.4547\n",
      "  Batch 20/31, Loss: 0.5212\n",
      "  Batch 30/31, Loss: 0.6951\n",
      "Epoch [206/1000], Loss: 0.7225\n",
      "Validation Loss: 0.7496\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000205.pth\n",
      "Deleted old checkpoint: epoch_000203.pth\n",
      "Starting epoch 207/1000\n",
      "  Batch 10/31, Loss: 0.7010\n",
      "  Batch 20/31, Loss: 0.4672\n",
      "  Batch 30/31, Loss: 0.5704\n",
      "Epoch [207/1000], Loss: 0.7483\n",
      "Validation Loss: 0.6356\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000206.pth\n",
      "Deleted old checkpoint: epoch_000204.pth\n",
      "Starting epoch 208/1000\n",
      "  Batch 10/31, Loss: 0.5334\n",
      "  Batch 20/31, Loss: 0.6986\n",
      "  Batch 30/31, Loss: 0.7521\n",
      "Epoch [208/1000], Loss: 0.6529\n",
      "Validation Loss: 0.6320\n",
      "Checkpoint saved to c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\visual_proprioception\\vp_convvae_128\\checkpoints\\epoch_000207.pth\n",
      "Deleted old checkpoint: epoch_000205.pth\n",
      "Starting epoch 209/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#if modelfile.exists():\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#    model.load_state_dict(torch.load(modelfile))\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain_and_save_proprioception_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 161\u001b[0m, in \u001b[0;36mtrain_and_save_proprioception_model\u001b[1;34m(exp)\u001b[0m\n\u001b[0;32m    159\u001b[0m     batch_X \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Standard single-view processing\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Make sure batch_y is on the same device\u001b[39;00m\n\u001b[0;32m    164\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_Checkouts\\BerryPicker\\src\\visual_proprioception\\..\\visual_proprioception\\visproprio_models.py:36\u001b[0m, in \u001b[0;36mVisProprio_SimpleMLPRegression.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:104\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torch\\nn\\functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\n",
    "\n",
    "#if modelfile.exists():\n",
    "#    model.load_state_dict(torch.load(modelfile))\n",
    "#else:\n",
    "train_and_save_proprioception_model(exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
