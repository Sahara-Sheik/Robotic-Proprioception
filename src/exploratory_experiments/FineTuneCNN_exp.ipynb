{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments for fine tuning a pre-trained CNN for sensor processing\n",
    "\n",
    "VGG-11 has a latent space of 512. How do we reduce / expand this to the latent space we want???\n",
    "\n",
    "* Idea 0: just use the features as it is\n",
    "* Idea 1: fine tune on proprioception\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Extracting the VGG-19 features from some pictures from a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from demonstration.demonstration_helper import BCDemonstration\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Load the pre-trained VGG-19 model\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "vgg19.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = \"vgg19_orig\"\n",
    "exp = Config().get_experiment(\"sp_cnn\", run)\n",
    "# model_subdir = Path(exp[\"data_dir\"], exp[\"model_dir\"], \"models\", exp[\"model_name\"], exp[\"model_subdir\"])\n",
    "# conv_vae_jsonfile = Path(model_subdir, \"config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"random-uncluttered\"\n",
    "demos_dir = Path(Config()[\"demos\"][\"directory\"])\n",
    "task_dir = Path(demos_dir, \"demos\", task)\n",
    "for demo_dir in task_dir.iterdir():\n",
    "    if not demo_dir.is_dir():\n",
    "        pass\n",
    "\n",
    "\n",
    "# These are the transforms that the image to what vgg-19 was trained on\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # Normalization for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "bcd = BCDemonstration(demo_dir, sensorprocessor=None)\n",
    "for i in range(1, bcd.maxsteps-1):\n",
    "    imgtensor, image = bcd.get_image(i, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgtensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg19.to(device)\n",
    "\n",
    "feature_extractor = vgg19.features\n",
    "#result = vgg19(imgtensor)\n",
    "result = feature_extractor(imgtensor)\n",
    "result.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1, constrained_layout=True)\n",
    "axs.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19Embedding(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(VGG19Embedding, self).__init__()\n",
    "        self.feature_extractor = vgg19.childrenfeatures\n",
    "        self.flatten = nn.Flatten()  # Flatten the output for the fully connected layer\n",
    "        self.fc = nn.Linear(512 * 7 * 7, latent_size)  # Adjust input size based on VGG19 output size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)\n",
    "        # Flatten the feature map\n",
    "        features_flat = self.flatten(features)\n",
    "        # Project to the latent space\n",
    "        latent = self.fc(features_flat)\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP regression model\n",
    "class VGG19Regression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(VGG19Regression, self).__init__()\n",
    "        vgg19 = models.vgg19(pretrained=True)\n",
    "        self.feature_extractor = vgg19.childrenfeatures\n",
    "        self.flatten = nn.Flatten()  # Flatten the output for the fully connected layer\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        # freeze the parameters\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False        \n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        flatfeatures = self.flatten(features)\n",
    "        return self.model(flatfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sp_conv_vae.get_sp_of_conv_vae_experiment(\"vae_01\")\n",
    "\n",
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = Path(exp[\"data_dir\"], \n",
    "                                            exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = Path(exp[\"data_dir\"], \n",
    "                                            exp[\"proprioception_target_file\"])\n",
    "\n",
    "# FIXME: this is going to be different, because we are going to be working on the original pictures, not on the extracted values\n",
    "\n",
    "tr = load_demonstrations_as_proprioception_training(sp, task, \n",
    "                                                    proprioception_input_file, proprioception_target_file)\n",
    "inputs_training = tr[\"inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "inputs_validation = tr[\"inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = inputs_training.size(1)\n",
    "hidden_size = 64\n",
    "output_size = targets_training.size(1)\n",
    "\n",
    "print(input_size)\n",
    "print(output_size)\n",
    "\n",
    "model = MLPRegression(input_size, hidden_size, output_size)\n",
    "# criterion = nn.MSELoss()\n",
    "# Experiment: would this be better???\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
