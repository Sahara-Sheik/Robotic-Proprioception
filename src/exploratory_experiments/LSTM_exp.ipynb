{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments in reproducing the LSTM architecture for robot control using Torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from gen_synthetic_data import generate_training_sequence, create_training_sequence_prediction, create_training_sequence_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "Generating predictable and unpredictable training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments in visualizing the generated training sequence\n",
    "total_length = 100\n",
    "# latent_size = 6\n",
    "x_seq, y_seq = generate_training_sequence(total_length=100, latent_size=7)\n",
    "plt.plot(y_seq[0:100,3], label= \"3\")\n",
    "plt.plot(x_seq[0:100,4], label= \"4\")\n",
    "plt.plot(x_seq[0:100,5], label= \"5\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence_length = 10  # Number of vectors in input sequence\n",
    "inputs, targets = create_training_sequence_prediction(x_seq, sequence_length=sequence_length)\n",
    "# Verify shapes\n",
    "print(\"Inputs shape:\", inputs.shape)   # Expected: [num_samples, sequence_length, latent_size]\n",
    "print(\"Targets shape:\", targets.shape) # Expected: [num_samples, latent_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model for sequence prediction\n",
    "Create an LSTM model for predicting a sequence. Train it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the architecture created by chatgpt\n",
    "class LSTMSequencePredictor(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, num_layers):\n",
    "        super(LSTMSequencePredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length, latent_size]\n",
    "        out, _ = self.lstm(x)  # LSTM output shape: [batch_size, sequence_length, hidden_size]\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step output and pass through the fully connected layer\n",
    "        return out  # Predicted next vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM controller with residuals and three layers\n",
    "# This code is written by Lotzi\n",
    "class LSTMResidualController(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size):\n",
    "        super(LSTMResidualController, self).__init__()\n",
    "        self.lstm_1 = nn.LSTM(latent_size, hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.lstm_2 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.lstm_3 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length, latent_size]\n",
    "        out_1, _ = self.lstm_1(x)\n",
    "        residual = out_1\n",
    "        out_2, _ = self.lstm_2(out_1)\n",
    "        out_2 = out_2 + residual\n",
    "        residual = out_2\n",
    "        out_3, _ = self.lstm_3(out_2)\n",
    "        out_3 = out_3 + residual\n",
    "\n",
    "        # LSTM output shape: [batch_size, sequence_length, hidden_size]\n",
    "        out = self.fc(out_3[:, -1, :])  # Take last time step output and pass through the fully connected layer\n",
    "        return out  # Predicted next vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Parameters\n",
    "# Original\n",
    "# latent_size = 7  # Dimension of each vector in the sequence\n",
    "# hidden_size = 32  # Size of hidden state in LSTM\n",
    "# num_layers = 2    # Number of LSTM layers\n",
    "\n",
    "# Original\n",
    "latent_size = 7  # Dimension of each vector in the sequence\n",
    "hidden_size = 3  # Size of hidden state in LSTM\n",
    "num_layers = 1    # Number of LSTM layers\n",
    "\n",
    "# Simple model: Instantiate model, loss function, and optimizer\n",
    "model = LSTMSequencePredictor(latent_size=latent_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "# Residual model: Instantiate model, loss function, and optimizer\n",
    "# model = LSTMResidualController(latent_size=latent_size, hidden_size=hidden_size, output_size = hidden_size)\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create the training data\n",
    "total_length = 100\n",
    "x_seq, y_seq = generate_training_sequence(total_length=total_length, latent_size=latent_size)\n",
    "inputs, targets = create_training_sequence_prediction(x_seq, sequence_length=sequence_length)\n",
    "num_sequences = inputs.shape[0]\n",
    "print(num_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each sequence in the batch\n",
    "    for i in range(num_sequences):\n",
    "        # Prepare input and target\n",
    "        input_seq = inputs[i]\n",
    "        target = targets[i]\n",
    "\n",
    "        # Reshape for batch compatibility\n",
    "        input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "        target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 2 == 0: # was 0\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# FIXME: save the model\n",
    "# FIXME: this should be saved into some experimental file name. \n",
    "filename_lstm = Config()[\"explorations\"][\"lstm_model_file\"]\n",
    "torch.save(model.state_dict(), filename_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "if load_model:\n",
    "    model = LSTMSequencePredictor(latent_size=latent_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "    filename_lstm = Config()[\"explorations\"][\"lstm_model_file\"]\n",
    "    model.load_state_dict(torch.load(filename_lstm))\n",
    "# FIXME: load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_step_by_step_prediction(model, sequence_length, latent_size):\n",
    "    \"\"\"Run a prediction where we are always predicting the next item from the previous 10 ground truth items\"\"\"\n",
    "    start = 0\n",
    "    result = np.zeros((sequence_length, latent_size))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, total_length - sequence_length):\n",
    "            input_seq = x_seq[start+i : start+ i + sequence_length]\n",
    "            input_val = input_seq.unsqueeze(0)\n",
    "            val = model(input_val)\n",
    "            result = np.append(result, [val.squeeze(0).tolist()], axis=0) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualize the step by step prediction\n",
    "result = model_step_by_step_prediction(model, sequence_length, latent_size)\n",
    "print(result.shape)\n",
    "\n",
    "channel = 2\n",
    "plt.plot(result[0:100,channel], label=f\"result {channel}\")\n",
    "plt.plot(x_seq[0:100,channel], label = f\"x_seq {channel}\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run autoregressive prediction (use the prediction as the next data)\n",
    "def model_autoregressive_prediction(model, x_seq, sequence_length):\n",
    "    \"\"\"Run an autoregressive prediction where we are always predicting the next item from the previous 10 ground truth items\"\"\"\n",
    "    start = 0\n",
    "    result = np.zeros((sequence_length, latent_size))\n",
    "    input_seq = x_seq[start : start + sequence_length]\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, total_length - sequence_length):\n",
    "            model.eval()            \n",
    "            input_val = input_seq.unsqueeze(0)\n",
    "            val = model(input_val)\n",
    "            next_item = val.squeeze(0).tolist()\n",
    "            result = np.append(result, [val.squeeze(0).tolist()], axis=0) \n",
    "            input_seq = input_seq[1: sequence_length]\n",
    "            input_seq= torch.cat((input_seq, val))\n",
    "    #print(result)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the autoregressive prediction\n",
    "# Calculate the error of the autoregressive prediction\n",
    "# Visualize the step by step prediction\n",
    "result = model_autoregressive_prediction(model, x_seq, sequence_length)\n",
    "channel = 4\n",
    "plt.plot(result[1:100,channel])\n",
    "plt.plot(x_seq[1:100,channel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with an output which is not prediction\n",
    "\n",
    "Training an LSTM with an output that is not a prediction of a string, but has a different dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the architecture created by chatgpt\n",
    "class LSTMXYPredictor(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMXYPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length, latent_size]\n",
    "        out, _ = self.lstm(x)  # LSTM output shape: [batch_size, sequence_length, hidden_size]\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step output and pass through the fully connected layer\n",
    "        return out  # Predicted next vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "# Original\n",
    "latent_size = 7  # Dimension of each vector in the sequence\n",
    "hidden_size = 32  # Size of hidden state in LSTM\n",
    "num_layers = 2    # Number of LSTM layers\n",
    "\n",
    "# Modified\n",
    "# latent_size = 7  # Dimension of each vector in the sequence\n",
    "# hidden_size = 3  # Size of hidden state in LSTM\n",
    "# num_layers = 1    # Number of LSTM layers\n",
    "output_size = 5\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMXYPredictor(latent_size=latent_size, hidden_size=hidden_size, output_size = output_size, num_layers=num_layers)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create the training data\n",
    "total_length = 100\n",
    "x_seq, y_seq = generate_training_sequence(total_length=total_length, latent_size=latent_size, y_size=output_size)\n",
    "inputs, targets = create_training_sequence_xy(x_seq, y_seq, sequence_length=sequence_length)\n",
    "num_sequences = inputs.shape[0]\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each sequence in the batch\n",
    "    for i in range(num_sequences):\n",
    "        # Prepare input and target\n",
    "        input_seq = inputs[i]\n",
    "        target = targets[i]\n",
    "\n",
    "        # Reshape for batch compatibility\n",
    "        input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "        target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 2 == 0: # was 0\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# FIXME: save the model\n",
    "filename_lstm = Config()[\"controller\"][\"lstm_model_file\"]\n",
    "torch.save(model.state_dict(), filename_lstm+\".2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: not implemented yet\n",
    "\n",
    "def model_xy_prediction(model, x_seq, sequence_length, latent_size, output_size):\n",
    "    \"\"\"Model the XY prediction\"\"\"\n",
    "    start = 0\n",
    "    result = np.zeros((sequence_length, output_size))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, total_length - sequence_length):\n",
    "            input_seq = x_seq[start+i : start+ i + sequence_length]\n",
    "            input_val = input_seq.unsqueeze(0)\n",
    "            val = model(input_val)\n",
    "            result = np.append(result, [val.squeeze(0).tolist()], axis=0) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the step by step prediction\n",
    "result = model_xy_prediction(model, x_seq, sequence_length, latent_size, output_size)\n",
    "print(result.shape)\n",
    "\n",
    "channel = 4\n",
    "plt.plot(result[0:100,channel], label=f\"result {channel}\")\n",
    "plt.plot(y_seq[0:100,channel], label = f\"y_seq {channel}\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
